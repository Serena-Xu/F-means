{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import skfuzzy as fuzz\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel('SAT_GPA.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1[['univ_GPA','SAT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a323e2f60>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df1['SAT'],df1['univ_GPA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>univ_GPA</th>\n",
       "      <th>SAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.52</td>\n",
       "      <td>1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.91</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.40</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.47</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.47</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.37</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.40</td>\n",
       "      <td>1121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.24</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.02</td>\n",
       "      <td>1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.32</td>\n",
       "      <td>1208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.59</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.54</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.19</td>\n",
       "      <td>1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.71</td>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.58</td>\n",
       "      <td>1261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.40</td>\n",
       "      <td>1325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.73</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.49</td>\n",
       "      <td>1364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.25</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.37</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.19</td>\n",
       "      <td>1161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.28</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.37</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.61</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.81</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.40</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.21</td>\n",
       "      <td>1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.58</td>\n",
       "      <td>1331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.51</td>\n",
       "      <td>1441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>3.51</td>\n",
       "      <td>1293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3.17</td>\n",
       "      <td>1155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>3.41</td>\n",
       "      <td>1379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3.29</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>3.17</td>\n",
       "      <td>1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>3.12</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>3.71</td>\n",
       "      <td>1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3.50</td>\n",
       "      <td>1293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>3.34</td>\n",
       "      <td>1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>3.48</td>\n",
       "      <td>1291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>3.44</td>\n",
       "      <td>1279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>3.59</td>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>3.28</td>\n",
       "      <td>1194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>3.00</td>\n",
       "      <td>1164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>3.42</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>3.41</td>\n",
       "      <td>1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3.49</td>\n",
       "      <td>1208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3.28</td>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3.17</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3.24</td>\n",
       "      <td>1374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2.34</td>\n",
       "      <td>1113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.28</td>\n",
       "      <td>1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2.29</td>\n",
       "      <td>1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2.08</td>\n",
       "      <td>1109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3.64</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>3.42</td>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>3.25</td>\n",
       "      <td>1372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>2.76</td>\n",
       "      <td>1120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>3.41</td>\n",
       "      <td>1044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     univ_GPA   SAT\n",
       "0        3.52  1232\n",
       "1        2.91  1070\n",
       "2        2.40  1086\n",
       "3        3.47  1287\n",
       "4        3.47  1130\n",
       "5        2.37  1048\n",
       "6        2.40  1121\n",
       "7        2.24  1095\n",
       "8        3.02  1135\n",
       "9        3.32  1208\n",
       "10       3.59  1333\n",
       "11       2.54  1160\n",
       "12       3.19  1186\n",
       "13       3.71  1243\n",
       "14       3.58  1261\n",
       "15       3.40  1325\n",
       "16       3.73  1387\n",
       "17       3.49  1364\n",
       "18       2.25  1065\n",
       "19       2.37  1106\n",
       "20       3.29  1175\n",
       "21       3.19  1161\n",
       "22       3.28  1226\n",
       "23       3.37  1177\n",
       "24       3.61  1421\n",
       "25       3.81  1450\n",
       "26       2.40  1118\n",
       "27       2.21  1069\n",
       "28       3.58  1331\n",
       "29       3.51  1441\n",
       "..        ...   ...\n",
       "75       3.51  1293\n",
       "76       3.17  1155\n",
       "77       3.20  1063\n",
       "78       3.41  1379\n",
       "79       3.29  1280\n",
       "80       3.17  1074\n",
       "81       3.12  1173\n",
       "82       3.71  1210\n",
       "83       3.50  1293\n",
       "84       3.34  1354\n",
       "85       3.48  1291\n",
       "86       3.44  1279\n",
       "87       3.59  1178\n",
       "88       3.28  1194\n",
       "89       3.00  1164\n",
       "90       3.42  1034\n",
       "91       3.41  1202\n",
       "92       3.49  1208\n",
       "93       3.28  1169\n",
       "94       3.17  1242\n",
       "95       3.24  1374\n",
       "96       2.34  1113\n",
       "97       3.28  1334\n",
       "98       2.29  1105\n",
       "99       2.08  1109\n",
       "100      3.64  1195\n",
       "101      3.42  1375\n",
       "102      3.25  1372\n",
       "103      2.76  1120\n",
       "104      3.41  1044\n",
       "\n",
       "[105 rows x 2 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2,init='k-means++',max_iter=300, tol=0.0001).fit(df1)\n",
    "classier=kmeans.labels_\n",
    "classier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2.94915254, 1137.27118644],\n",
       "       [   3.45978261, 1329.93478261]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'University GPA')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "red=[]\n",
    "green=[]\n",
    "redy=[]\n",
    "greeny=[]\n",
    "redcount=0\n",
    "greencount=0\n",
    "for i in range(len(df1)):\n",
    "    if classier[i]==0:\n",
    "        plt.scatter(df1['SAT'][i],df1['univ_GPA'][i],color='red')\n",
    "        red.append(df1['SAT'][i])\n",
    "        redy.append(df1['univ_GPA'][i])\n",
    "        redcount+=1\n",
    "    else:\n",
    "        plt.scatter(df1['SAT'][i],df1['univ_GPA'][i],color='green') \n",
    "        green.append(df1['SAT'][i])\n",
    "        greeny.append(df1['univ_GPA'][i])\n",
    "        greencount+=1\n",
    "plt.xlabel('SAT')\n",
    "plt.ylabel('University GPA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat1=np.arange(1034,1137.3,0.02)\n",
    "sat2=np.arange(1089.95,1232,0.02)\n",
    "sat3=np.arange(1242,1373.85,0.02)\n",
    "sat4=np.arange(1329.9,1450,0.02)\n",
    "gpa=np.arange(2,4,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate membership functions\n",
    "sat1_lo=fuzz.trimf(sat1,[1034,1034,1090])\n",
    "sat1_md=fuzz.trimf(sat1,[1034,1090,1137.3])\n",
    "sat1_hi=fuzz.trimf(sat1,[1090,1137.3,1137.3])\n",
    "\n",
    "sat2_lo=fuzz.trimf(sat2,[1089.95,1089.95,1137.30])\n",
    "sat2_md=fuzz.trimf(sat2,[1089.95,1137.30,1232.00])\n",
    "sat2_hi=fuzz.trimf(sat2,[1137.30,1232.00,1232.00])\n",
    "\n",
    "sat3_lo=fuzz.trimf(sat3,[1242,1242,1329.9])\n",
    "sat3_md=fuzz.trimf(sat3,[1242,1329.9,1373.85])\n",
    "sat3_hi=fuzz.trimf(sat3,[1090,1137.3,1137.3])\n",
    "\n",
    "sat4_lo=fuzz.trimf(sat4,[0,0,1329.9])\n",
    "sat4_md=fuzz.trimf(sat4,[1329.9,1340.6,1450])\n",
    "sat4_hi=fuzz.trimf(sat4,[1340.6,1450,1450])\n",
    "\n",
    "gpa1_lo=fuzz.trimf(gpa,[2.08,2.08,2.74])\n",
    "gpa1_md=fuzz.trimf(gpa,[2.08,2.74,2.95])\n",
    "gpa1_hi=fuzz.trimf(gpa,[2.74,2.95,2.95])\n",
    "\n",
    "gpa2_lo=fuzz.trimf(gpa,[2.57,2.57,2.95])\n",
    "gpa2_md=fuzz.trimf(gpa,[2.57,2.95,3.71])\n",
    "gpa2_hi=fuzz.trimf(gpa,[2.95,3.71,3.71])\n",
    "\n",
    "gpa3_lo=fuzz.trimf(gpa,[3.01,3.01,3.35])\n",
    "gpa3_md=fuzz.trimf(gpa,[3.01,3.35,3.46])\n",
    "gpa3_hi=fuzz.trimf(gpa,[3.35,3.46,3.46])\n",
    "\n",
    "gpa4_lo=fuzz.trimf(gpa,[3.285,3.285,3.46])\n",
    "gpa4_md=fuzz.trimf(gpa,[3.285,3.46,3.81])\n",
    "gpa4_hi=fuzz.trimf(gpa,[3.46,3.81,3.81])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT1=np.concatenate((sat1,sat2))\n",
    "# SAT1_lo=np.concatenate((sat1_lo,sat2_lo))\n",
    "# SAT1_md=np.concatenate((sat1_md,sat2_md))\n",
    "# SAT1_hi=np.concatenate((sat1_hi,sat2_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAT2=np.concatenate((sat3,sat4))\n",
    "# SAT2_lo=np.concatenate((sat3_lo,sat4_lo))\n",
    "# SAT2_md=np.concatenate((sat3_md,sat4_md))\n",
    "# SAT2_hi=np.concatenate((sat3_hi,sat4_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rule application\n",
    "sa1_lo=fuzz.interp_membership(sat3,sat3_lo,1364)\n",
    "sa1_md=fuzz.interp_membership(sat3,sat3_md,1364)\n",
    "sa1_hi=fuzz.interp_membership(sat3,sat3_hi,1364)\n",
    "\n",
    "sa2_lo=fuzz.interp_membership(sat4,sat4_lo,1364)\n",
    "sa2_md=fuzz.interp_membership(sat4,sat4_md,1364)\n",
    "sa2_hi=fuzz.interp_membership(sat4,sat4_hi,1364)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule1_lo=np.fmin(np.fmax(sa1_lo,sa2_lo),np.fmax(gpa3_lo,gpa4_lo))\n",
    "rule1_md=np.fmin(np.fmax(sa1_md,sa2_md),np.fmax(gpa3_md,gpa4_md))\n",
    "rule1_hi=np.fmin(np.fmax(sa1_hi,sa2_hi),np.fmax(gpa3_hi,gpa4_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated=np.fmax(rule1_lo,np.fmax(rule1_md,rule1_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp=fuzz.defuzz(gpa,aggregated,'centroid')\n",
    "gpa=fuzz.interp_membership(gpa,aggregated,gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.416722832181515"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df1['SAT']\n",
    "target=df1['univ_GPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = data[81:]\n",
    "test_y = target[81:]\n",
    "train_x = data[:81]\n",
    "train_y = target[:81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 5,251\n",
      "Trainable params: 5,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 81 samples, validate on 24 samples\n",
      "Epoch 1/2000\n",
      "81/81 [==============================] - 1s 17ms/step - loss: 22280.6836 - val_loss: 8547.5537\n",
      "Epoch 2/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 16422.5820 - val_loss: 5378.5396\n",
      "Epoch 3/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 10167.6777 - val_loss: 3192.0286\n",
      "Epoch 4/2000\n",
      "81/81 [==============================] - 0s 111us/step - loss: 7643.7251 - val_loss: 1756.7740\n",
      "Epoch 5/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 5225.1558 - val_loss: 893.4813\n",
      "Epoch 6/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 4838.7720 - val_loss: 349.6522\n",
      "Epoch 7/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 3093.8047 - val_loss: 58.6384\n",
      "Epoch 8/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 2544.7966 - val_loss: 6.2118\n",
      "Epoch 9/2000\n",
      "81/81 [==============================] - 0s 107us/step - loss: 1859.6400 - val_loss: 141.1402\n",
      "Epoch 10/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 1958.1915 - val_loss: 409.6341\n",
      "Epoch 11/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 1966.9861 - val_loss: 712.2254\n",
      "Epoch 12/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 2366.9482 - val_loss: 941.2083\n",
      "Epoch 13/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 2541.8921 - val_loss: 1126.6012\n",
      "Epoch 14/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 2579.0520 - val_loss: 1251.7112\n",
      "Epoch 15/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 2273.8838 - val_loss: 1234.9703\n",
      "Epoch 16/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 3227.0081 - val_loss: 1120.8193\n",
      "Epoch 17/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 2212.4443 - val_loss: 993.4709\n",
      "Epoch 18/2000\n",
      "81/81 [==============================] - 0s 102us/step - loss: 1527.4830 - val_loss: 861.6558\n",
      "Epoch 19/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 1649.9039 - val_loss: 731.5706\n",
      "Epoch 20/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 1348.4705 - val_loss: 618.5255\n",
      "Epoch 21/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 1839.3547 - val_loss: 515.1980\n",
      "Epoch 22/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 1079.1055 - val_loss: 417.8167\n",
      "Epoch 23/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 1264.8490 - val_loss: 332.7885\n",
      "Epoch 24/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 980.7606 - val_loss: 259.4325\n",
      "Epoch 25/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 586.1231 - val_loss: 197.3362\n",
      "Epoch 26/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 594.2000 - val_loss: 145.5932\n",
      "Epoch 27/2000\n",
      "81/81 [==============================] - 0s 107us/step - loss: 714.0437 - val_loss: 102.5916\n",
      "Epoch 28/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 703.7468 - val_loss: 68.1711\n",
      "Epoch 29/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 469.0443 - val_loss: 42.9705\n",
      "Epoch 30/2000\n",
      "81/81 [==============================] - 0s 105us/step - loss: 511.0467 - val_loss: 24.7493\n",
      "Epoch 31/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 684.0623 - val_loss: 12.6907\n",
      "Epoch 32/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 588.7214 - val_loss: 5.2335\n",
      "Epoch 33/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 400.4996 - val_loss: 1.4567\n",
      "Epoch 34/2000\n",
      "81/81 [==============================] - 0s 107us/step - loss: 446.8104 - val_loss: 0.1877\n",
      "Epoch 35/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 544.7379 - val_loss: 0.5879\n",
      "Epoch 36/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 591.4510 - val_loss: 1.9876\n",
      "Epoch 37/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 364.8385 - val_loss: 3.9000\n",
      "Epoch 38/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 502.3264 - val_loss: 5.8652\n",
      "Epoch 39/2000\n",
      "81/81 [==============================] - 0s 126us/step - loss: 494.4554 - val_loss: 7.2377\n",
      "Epoch 40/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 490.7896 - val_loss: 8.2684\n",
      "Epoch 41/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 386.2474 - val_loss: 8.6671\n",
      "Epoch 42/2000\n",
      "81/81 [==============================] - 0s 109us/step - loss: 554.0874 - val_loss: 8.3637\n",
      "Epoch 43/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 357.6939 - val_loss: 7.8987\n",
      "Epoch 44/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 398.6729 - val_loss: 7.0145\n",
      "Epoch 45/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 361.3805 - val_loss: 5.6394\n",
      "Epoch 46/2000\n",
      "81/81 [==============================] - 0s 118us/step - loss: 315.1604 - val_loss: 4.1444\n",
      "Epoch 47/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 499.4827 - val_loss: 2.9461\n",
      "Epoch 48/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 379.0693 - val_loss: 1.9353\n",
      "Epoch 49/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 306.7263 - val_loss: 1.2390\n",
      "Epoch 50/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 379.1390 - val_loss: 0.7013\n",
      "Epoch 51/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 349.7979 - val_loss: 0.3557\n",
      "Epoch 52/2000\n",
      "81/81 [==============================] - 0s 95us/step - loss: 376.2458 - val_loss: 0.1692\n",
      "Epoch 53/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 271.3374 - val_loss: 0.2079\n",
      "Epoch 54/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 288.0293 - val_loss: 0.3810\n",
      "Epoch 55/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 378.3978 - val_loss: 0.6639\n",
      "Epoch 56/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 393.8976 - val_loss: 1.0797\n",
      "Epoch 57/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 314.0721 - val_loss: 1.5964\n",
      "Epoch 58/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 277.4115 - val_loss: 1.8597\n",
      "Epoch 59/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 181.2916 - val_loss: 1.9542\n",
      "Epoch 60/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 342.3799 - val_loss: 1.8960\n",
      "Epoch 61/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 232.5873 - val_loss: 1.6205\n",
      "Epoch 62/2000\n",
      "81/81 [==============================] - 0s 200us/step - loss: 238.1621 - val_loss: 1.2867\n",
      "Epoch 63/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 195.4754 - val_loss: 1.0056\n",
      "Epoch 64/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 310.7996 - val_loss: 0.6820\n",
      "Epoch 65/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 278.6985 - val_loss: 0.4131\n",
      "Epoch 66/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 364.6023 - val_loss: 0.2210\n",
      "Epoch 67/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 326.8537 - val_loss: 0.1591\n",
      "Epoch 68/2000\n",
      "81/81 [==============================] - 0s 108us/step - loss: 439.4263 - val_loss: 0.2459\n",
      "Epoch 69/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 232.0413 - val_loss: 0.5984\n",
      "Epoch 70/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 225.2837 - val_loss: 1.2068\n",
      "Epoch 71/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 243.0175 - val_loss: 2.0810\n",
      "Epoch 72/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 217.4059 - val_loss: 3.1346\n",
      "Epoch 73/2000\n",
      "81/81 [==============================] - 0s 134us/step - loss: 291.1032 - val_loss: 4.1415\n",
      "Epoch 74/2000\n",
      "81/81 [==============================] - 0s 98us/step - loss: 212.1414 - val_loss: 5.3824\n",
      "Epoch 75/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 290.9348 - val_loss: 6.6127\n",
      "Epoch 76/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 236.4962 - val_loss: 7.8520\n",
      "Epoch 77/2000\n",
      "81/81 [==============================] - 0s 125us/step - loss: 263.3482 - val_loss: 8.9953\n",
      "Epoch 78/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 327.9225 - val_loss: 9.7855\n",
      "Epoch 79/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 211.1502 - val_loss: 10.6653\n",
      "Epoch 80/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 222.0633 - val_loss: 10.9901\n",
      "Epoch 81/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 159.1152 - val_loss: 11.2626\n",
      "Epoch 82/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 229.9123 - val_loss: 11.3025\n",
      "Epoch 83/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 226.9970 - val_loss: 11.3874\n",
      "Epoch 84/2000\n",
      "81/81 [==============================] - 0s 102us/step - loss: 220.1301 - val_loss: 11.3428\n",
      "Epoch 85/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 197.2822 - val_loss: 11.4558\n",
      "Epoch 86/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 218.7420 - val_loss: 11.9644\n",
      "Epoch 87/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 147.7603 - val_loss: 12.3159\n",
      "Epoch 88/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 158.9417 - val_loss: 12.6805\n",
      "Epoch 89/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 159.9653 - val_loss: 12.5213\n",
      "Epoch 90/2000\n",
      "81/81 [==============================] - 0s 105us/step - loss: 274.7949 - val_loss: 12.5195\n",
      "Epoch 91/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 248.2716 - val_loss: 12.5182\n",
      "Epoch 92/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 175.0391 - val_loss: 12.7309\n",
      "Epoch 93/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 327.1484 - val_loss: 12.7823\n",
      "Epoch 94/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 239.7055 - val_loss: 12.8114\n",
      "Epoch 95/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 158.7296 - val_loss: 12.5092\n",
      "Epoch 96/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 194.7177 - val_loss: 12.3481\n",
      "Epoch 97/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 105.5397 - val_loss: 12.1019\n",
      "Epoch 98/2000\n",
      "81/81 [==============================] - 0s 112us/step - loss: 182.3553 - val_loss: 11.7673\n",
      "Epoch 99/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 176.5633 - val_loss: 11.5769\n",
      "Epoch 100/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 187.8434 - val_loss: 11.5363\n",
      "Epoch 101/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 169.1686 - val_loss: 11.3670\n",
      "Epoch 102/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 192.8228 - val_loss: 11.2606\n",
      "Epoch 103/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 206.4570 - val_loss: 10.9638\n",
      "Epoch 104/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 134.1414 - val_loss: 10.7209\n",
      "Epoch 105/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 218.5943 - val_loss: 10.5092\n",
      "Epoch 106/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 137.2474 - val_loss: 10.4134\n",
      "Epoch 107/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 171.8442 - val_loss: 10.2880\n",
      "Epoch 108/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 165.2064 - val_loss: 9.8276\n",
      "Epoch 109/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 136.9457 - val_loss: 9.4815\n",
      "Epoch 110/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 134.4776 - val_loss: 8.8902\n",
      "Epoch 111/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 200.2861 - val_loss: 8.4216\n",
      "Epoch 112/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 174.3018 - val_loss: 8.1132\n",
      "Epoch 113/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 160.8122 - val_loss: 7.9043\n",
      "Epoch 114/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 126.7521 - val_loss: 7.7569\n",
      "Epoch 115/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 219.1898 - val_loss: 7.7454\n",
      "Epoch 116/2000\n",
      "81/81 [==============================] - 0s 106us/step - loss: 148.1395 - val_loss: 7.6077\n",
      "Epoch 117/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 219.7377 - val_loss: 7.5206\n",
      "Epoch 118/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 154.7429 - val_loss: 7.5725\n",
      "Epoch 119/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 121.2775 - val_loss: 7.5850\n",
      "Epoch 120/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 170.5229 - val_loss: 7.8181\n",
      "Epoch 121/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 161.7162 - val_loss: 8.1351\n",
      "Epoch 122/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 146.1604 - val_loss: 8.4576\n",
      "Epoch 123/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 94.6957 - val_loss: 8.8292\n",
      "Epoch 124/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 167.5161 - val_loss: 9.7167\n",
      "Epoch 125/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 127.9981 - val_loss: 10.7199\n",
      "Epoch 126/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 121.5758 - val_loss: 11.2796\n",
      "Epoch 127/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 210.8309 - val_loss: 11.6469\n",
      "Epoch 128/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 171.2157 - val_loss: 12.3140\n",
      "Epoch 129/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 226.6628 - val_loss: 13.4672\n",
      "Epoch 130/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 143.5829 - val_loss: 14.8174\n",
      "Epoch 131/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 156.8389 - val_loss: 15.7575\n",
      "Epoch 132/2000\n",
      "81/81 [==============================] - 0s 95us/step - loss: 130.2530 - val_loss: 16.7468\n",
      "Epoch 133/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 171.5088 - val_loss: 17.6015\n",
      "Epoch 134/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 225.1522 - val_loss: 19.0242\n",
      "Epoch 135/2000\n",
      "81/81 [==============================] - 0s 106us/step - loss: 112.7973 - val_loss: 20.1231\n",
      "Epoch 136/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 265.5890 - val_loss: 21.0448\n",
      "Epoch 137/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 109.6633 - val_loss: 21.3027\n",
      "Epoch 138/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 131.4761 - val_loss: 21.8888\n",
      "Epoch 139/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 170.4341 - val_loss: 22.2887\n",
      "Epoch 140/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 161.8837 - val_loss: 22.7621\n",
      "Epoch 141/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 108.3408 - val_loss: 23.2234\n",
      "Epoch 142/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 156.4541 - val_loss: 23.3444\n",
      "Epoch 143/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 179.8483 - val_loss: 22.9633\n",
      "Epoch 144/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 196.1434 - val_loss: 22.1302\n",
      "Epoch 145/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 123.9972 - val_loss: 21.0264\n",
      "Epoch 146/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 148.6368 - val_loss: 19.5673\n",
      "Epoch 147/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 114.6158 - val_loss: 18.3156\n",
      "Epoch 148/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 192.4425 - val_loss: 16.7423\n",
      "Epoch 149/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 166.6203 - val_loss: 15.1122\n",
      "Epoch 150/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 165.2985 - val_loss: 13.5706\n",
      "Epoch 151/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 177.4599 - val_loss: 12.2993\n",
      "Epoch 152/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 154.1166 - val_loss: 10.9953\n",
      "Epoch 153/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 163.5034 - val_loss: 9.9130\n",
      "Epoch 154/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 131.2626 - val_loss: 9.2379\n",
      "Epoch 155/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 111.8001 - val_loss: 8.6886\n",
      "Epoch 156/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 141.7539 - val_loss: 8.3287\n",
      "Epoch 157/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 125.2207 - val_loss: 7.7699\n",
      "Epoch 158/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 148.8789 - val_loss: 7.2784\n",
      "Epoch 159/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 204.5118 - val_loss: 7.2333\n",
      "Epoch 160/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 80.1613 - val_loss: 7.3339\n",
      "Epoch 161/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 144.1954 - val_loss: 7.7334\n",
      "Epoch 162/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 132.5064 - val_loss: 8.5105\n",
      "Epoch 163/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 146.3597 - val_loss: 9.3142\n",
      "Epoch 164/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 136.4136 - val_loss: 9.9940\n",
      "Epoch 165/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 90.3770 - val_loss: 10.3792\n",
      "Epoch 166/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 94.7085 - val_loss: 11.0314\n",
      "Epoch 167/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 131.4487 - val_loss: 11.6235\n",
      "Epoch 168/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 101.6144 - val_loss: 11.8972\n",
      "Epoch 169/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 170.5755 - val_loss: 12.1198\n",
      "Epoch 170/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 167.1194 - val_loss: 12.1388\n",
      "Epoch 171/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 70.6512 - val_loss: 12.0529\n",
      "Epoch 172/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 137.4037 - val_loss: 11.8764\n",
      "Epoch 173/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 110.3335 - val_loss: 11.5574\n",
      "Epoch 174/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 110.1436 - val_loss: 11.5446\n",
      "Epoch 175/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 112.6711 - val_loss: 11.7933\n",
      "Epoch 176/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 78.7854 - val_loss: 11.7887\n",
      "Epoch 177/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 134.5382 - val_loss: 11.4663\n",
      "Epoch 178/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 107.3859 - val_loss: 11.2536\n",
      "Epoch 179/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 112.4643 - val_loss: 10.8535\n",
      "Epoch 180/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 102.5984 - val_loss: 10.5079\n",
      "Epoch 181/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 98.3491 - val_loss: 10.2825\n",
      "Epoch 182/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 118.9614 - val_loss: 9.8919\n",
      "Epoch 183/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 106.6688 - val_loss: 9.7571\n",
      "Epoch 184/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 115.8553 - val_loss: 9.5032\n",
      "Epoch 185/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 91.8546 - val_loss: 9.2356\n",
      "Epoch 186/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 85.8053 - val_loss: 8.8196\n",
      "Epoch 187/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 69.0886 - val_loss: 8.4774\n",
      "Epoch 188/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 106.8255 - val_loss: 8.2723\n",
      "Epoch 189/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 110.8139 - val_loss: 7.8228\n",
      "Epoch 190/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 103.3353 - val_loss: 7.4912\n",
      "Epoch 191/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 77.5187 - val_loss: 7.0454\n",
      "Epoch 192/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 107.7355 - val_loss: 6.8111\n",
      "Epoch 193/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 102.8275 - val_loss: 6.6747\n",
      "Epoch 194/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 106.0149 - val_loss: 6.8792\n",
      "Epoch 195/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 83.8560 - val_loss: 7.0056\n",
      "Epoch 196/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 147.8030 - val_loss: 7.3025\n",
      "Epoch 197/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 104.7216 - val_loss: 7.4715\n",
      "Epoch 198/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 94.3665 - val_loss: 7.8583\n",
      "Epoch 199/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 87.6614 - val_loss: 8.1892\n",
      "Epoch 200/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 88.9820 - val_loss: 8.3334\n",
      "Epoch 201/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 141.2885 - val_loss: 9.0736\n",
      "Epoch 202/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 82.0266 - val_loss: 9.9248\n",
      "Epoch 203/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 98.3956 - val_loss: 10.3000\n",
      "Epoch 204/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 115.4810 - val_loss: 10.5478\n",
      "Epoch 205/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 78.1441 - val_loss: 11.0617\n",
      "Epoch 206/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 66.4350 - val_loss: 11.7436\n",
      "Epoch 207/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 66.2863 - val_loss: 12.8088\n",
      "Epoch 208/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 91.6729 - val_loss: 13.5165\n",
      "Epoch 209/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 84.4440 - val_loss: 14.0940\n",
      "Epoch 210/2000\n",
      "81/81 [==============================] - 0s 111us/step - loss: 75.0664 - val_loss: 14.4467\n",
      "Epoch 211/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 101.4100 - val_loss: 14.3623\n",
      "Epoch 212/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 89.6489 - val_loss: 14.2564\n",
      "Epoch 213/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 81.2095 - val_loss: 13.8959\n",
      "Epoch 214/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 103.7150 - val_loss: 13.7955\n",
      "Epoch 215/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 68.1357 - val_loss: 13.4265\n",
      "Epoch 216/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 85.8874 - val_loss: 13.0766\n",
      "Epoch 217/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 74.4457 - val_loss: 12.9870\n",
      "Epoch 218/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 87.4561 - val_loss: 12.5828\n",
      "Epoch 219/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 133.9603 - val_loss: 12.2495\n",
      "Epoch 220/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 77.8291 - val_loss: 11.8333\n",
      "Epoch 221/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 100.2564 - val_loss: 11.5382\n",
      "Epoch 222/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 132.4142 - val_loss: 11.4641\n",
      "Epoch 223/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 42.8655 - val_loss: 11.2988\n",
      "Epoch 224/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 116.6643 - val_loss: 11.2129\n",
      "Epoch 225/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 87.8030 - val_loss: 11.2092\n",
      "Epoch 226/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 55.4830 - val_loss: 11.1787\n",
      "Epoch 227/2000\n",
      "81/81 [==============================] - 0s 126us/step - loss: 56.2524 - val_loss: 11.4830\n",
      "Epoch 228/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 82.7177 - val_loss: 12.0248\n",
      "Epoch 229/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 83.4397 - val_loss: 12.6033\n",
      "Epoch 230/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 90.2482 - val_loss: 12.5125\n",
      "Epoch 231/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 87.9716 - val_loss: 12.3216\n",
      "Epoch 232/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 64.3921 - val_loss: 11.9131\n",
      "Epoch 233/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 82.7523 - val_loss: 11.6280\n",
      "Epoch 234/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 73.3182 - val_loss: 11.0391\n",
      "Epoch 235/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 98.6255 - val_loss: 10.8115\n",
      "Epoch 236/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 79.3791 - val_loss: 10.6012\n",
      "Epoch 237/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 96.1834 - val_loss: 9.9160\n",
      "Epoch 238/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 102.0547 - val_loss: 9.4391\n",
      "Epoch 239/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 75.3661 - val_loss: 8.9944\n",
      "Epoch 240/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 82.1307 - val_loss: 8.7881\n",
      "Epoch 241/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 61.5546 - val_loss: 8.7588\n",
      "Epoch 242/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 104.8693 - val_loss: 8.9790\n",
      "Epoch 243/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 78.2400 - val_loss: 9.1218\n",
      "Epoch 244/2000\n",
      "81/81 [==============================] - 0s 136us/step - loss: 85.0505 - val_loss: 9.3202\n",
      "Epoch 245/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 72.5147 - val_loss: 9.4781\n",
      "Epoch 246/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 62.1664 - val_loss: 9.4421\n",
      "Epoch 247/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 71.7281 - val_loss: 9.5039\n",
      "Epoch 248/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 55.5703 - val_loss: 9.5805\n",
      "Epoch 249/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 66.2901 - val_loss: 9.9523\n",
      "Epoch 250/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 67.6094 - val_loss: 9.9830\n",
      "Epoch 251/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 83.2282 - val_loss: 9.9490\n",
      "Epoch 252/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 86.2989 - val_loss: 9.8221\n",
      "Epoch 253/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 85.0155 - val_loss: 9.7185\n",
      "Epoch 254/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 50.0984 - val_loss: 9.5985\n",
      "Epoch 255/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 96.2234 - val_loss: 9.6534\n",
      "Epoch 256/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 77.2232 - val_loss: 9.7605\n",
      "Epoch 257/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 95.1932 - val_loss: 9.6739\n",
      "Epoch 258/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 58.7872 - val_loss: 9.7219\n",
      "Epoch 259/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 126.0179 - val_loss: 9.3281\n",
      "Epoch 260/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 82.2146 - val_loss: 8.7529\n",
      "Epoch 261/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 46.7273 - val_loss: 8.0546\n",
      "Epoch 262/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 87.4842 - val_loss: 7.6034\n",
      "Epoch 263/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 82.3735 - val_loss: 7.1941\n",
      "Epoch 264/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 84.3489 - val_loss: 6.6828\n",
      "Epoch 265/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 53.7029 - val_loss: 6.2542\n",
      "Epoch 266/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 59.5791 - val_loss: 5.7146\n",
      "Epoch 267/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 94.0729 - val_loss: 5.2666\n",
      "Epoch 268/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 67.5895 - val_loss: 5.1542\n",
      "Epoch 269/2000\n",
      "81/81 [==============================] - 0s 115us/step - loss: 73.8884 - val_loss: 5.3660\n",
      "Epoch 270/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 59.6732 - val_loss: 5.8777\n",
      "Epoch 271/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 44.8874 - val_loss: 6.4659\n",
      "Epoch 272/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 96.1493 - val_loss: 7.2645\n",
      "Epoch 273/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 55.0743 - val_loss: 7.8985\n",
      "Epoch 274/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 70.2285 - val_loss: 8.5266\n",
      "Epoch 275/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 99.1808 - val_loss: 9.3982\n",
      "Epoch 276/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 56.7617 - val_loss: 10.3488\n",
      "Epoch 277/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 59.5486 - val_loss: 11.1238\n",
      "Epoch 278/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 87.0162 - val_loss: 11.6655\n",
      "Epoch 279/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 68.9694 - val_loss: 12.1740\n",
      "Epoch 280/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 66.1216 - val_loss: 12.1110\n",
      "Epoch 281/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 72.3113 - val_loss: 11.7160\n",
      "Epoch 282/2000\n",
      "81/81 [==============================] - 0s 107us/step - loss: 53.1197 - val_loss: 11.2790\n",
      "Epoch 283/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 73.4408 - val_loss: 10.5846\n",
      "Epoch 284/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 56.3040 - val_loss: 9.8963\n",
      "Epoch 285/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 81.0341 - val_loss: 8.9621\n",
      "Epoch 286/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 64.0061 - val_loss: 7.7208\n",
      "Epoch 287/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 89.8032 - val_loss: 6.4416\n",
      "Epoch 288/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 50.8351 - val_loss: 5.6700\n",
      "Epoch 289/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 53.2458 - val_loss: 5.0611\n",
      "Epoch 290/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 65.9306 - val_loss: 4.7385\n",
      "Epoch 291/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 72.2366 - val_loss: 4.6654\n",
      "Epoch 292/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 112.8074 - val_loss: 4.4960\n",
      "Epoch 293/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 63.2491 - val_loss: 4.4958\n",
      "Epoch 294/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 57.4610 - val_loss: 4.2391\n",
      "Epoch 295/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 36.2284 - val_loss: 4.1545\n",
      "Epoch 296/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 98.6785 - val_loss: 4.0226\n",
      "Epoch 297/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 50.5760 - val_loss: 4.0318\n",
      "Epoch 298/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 42.1402 - val_loss: 3.8980\n",
      "Epoch 299/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 47.6251 - val_loss: 3.8768\n",
      "Epoch 300/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 42.1935 - val_loss: 3.7635\n",
      "Epoch 301/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 84.0112 - val_loss: 3.6333\n",
      "Epoch 302/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 100.7078 - val_loss: 3.2795\n",
      "Epoch 303/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 32.1228 - val_loss: 3.0789\n",
      "Epoch 304/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 62.0373 - val_loss: 2.7832\n",
      "Epoch 305/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 76.6748 - val_loss: 2.5496\n",
      "Epoch 306/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 79.6623 - val_loss: 2.5537\n",
      "Epoch 307/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 56.8228 - val_loss: 2.7185\n",
      "Epoch 308/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 58.6065 - val_loss: 2.9439\n",
      "Epoch 309/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 59.7844 - val_loss: 3.1879\n",
      "Epoch 310/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 58.7542 - val_loss: 3.5211\n",
      "Epoch 311/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 99.0105 - val_loss: 4.1189\n",
      "Epoch 312/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 34.6066 - val_loss: 4.7837\n",
      "Epoch 313/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 54.0540 - val_loss: 5.3874\n",
      "Epoch 314/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 36.8788 - val_loss: 5.7409\n",
      "Epoch 315/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 47.4180 - val_loss: 6.0108\n",
      "Epoch 316/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 64.6113 - val_loss: 5.9393\n",
      "Epoch 317/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 58.9611 - val_loss: 5.5621\n",
      "Epoch 318/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 57.4160 - val_loss: 5.0665\n",
      "Epoch 319/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 63.6985 - val_loss: 4.5198\n",
      "Epoch 320/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 36.5840 - val_loss: 4.0267\n",
      "Epoch 321/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 72.2590 - val_loss: 3.6842\n",
      "Epoch 322/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 65.8768 - val_loss: 3.5294\n",
      "Epoch 323/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 48.1973 - val_loss: 3.3287\n",
      "Epoch 324/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 73.8211 - val_loss: 3.2181\n",
      "Epoch 325/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 70.3759 - val_loss: 3.0426\n",
      "Epoch 326/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 45.6296 - val_loss: 3.0027\n",
      "Epoch 327/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 52.7651 - val_loss: 2.8395\n",
      "Epoch 328/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 34.5010 - val_loss: 2.8216\n",
      "Epoch 329/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 41.0764 - val_loss: 2.8735\n",
      "Epoch 330/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 49.2121 - val_loss: 2.8904\n",
      "Epoch 331/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 58.3597 - val_loss: 2.8064\n",
      "Epoch 332/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 37.2444 - val_loss: 2.6204\n",
      "Epoch 333/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 86.9451 - val_loss: 2.6424\n",
      "Epoch 334/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 78.3463 - val_loss: 2.6004\n",
      "Epoch 335/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 42.5430 - val_loss: 2.5773\n",
      "Epoch 336/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 55.9879 - val_loss: 2.6350\n",
      "Epoch 337/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 40.7164 - val_loss: 2.7256\n",
      "Epoch 338/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 64.2786 - val_loss: 2.8348\n",
      "Epoch 339/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 31.7197 - val_loss: 2.9686\n",
      "Epoch 340/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 61.1035 - val_loss: 3.0534\n",
      "Epoch 341/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 55.0868 - val_loss: 3.2112\n",
      "Epoch 342/2000\n",
      "81/81 [==============================] - 0s 98us/step - loss: 34.9053 - val_loss: 3.3613\n",
      "Epoch 343/2000\n",
      "81/81 [==============================] - 0s 130us/step - loss: 60.2465 - val_loss: 3.3449\n",
      "Epoch 344/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 51.7093 - val_loss: 3.3924\n",
      "Epoch 345/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 54.7431 - val_loss: 3.3585\n",
      "Epoch 346/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 58.1124 - val_loss: 3.3751\n",
      "Epoch 347/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 37.4769 - val_loss: 3.4750\n",
      "Epoch 348/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 48.6946 - val_loss: 3.5741\n",
      "Epoch 349/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 48.5879 - val_loss: 3.7986\n",
      "Epoch 350/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 36.4160 - val_loss: 3.9185\n",
      "Epoch 351/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 59.3095 - val_loss: 3.9843\n",
      "Epoch 352/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 48.4804 - val_loss: 3.9446\n",
      "Epoch 353/2000\n",
      "81/81 [==============================] - 0s 136us/step - loss: 51.9101 - val_loss: 3.8246\n",
      "Epoch 354/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 55.3963 - val_loss: 3.6895\n",
      "Epoch 355/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 46.9594 - val_loss: 3.5669\n",
      "Epoch 356/2000\n",
      "81/81 [==============================] - 0s 146us/step - loss: 48.3201 - val_loss: 3.3904\n",
      "Epoch 357/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 58.2834 - val_loss: 3.0898\n",
      "Epoch 358/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 46.8796 - val_loss: 2.8310\n",
      "Epoch 359/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 38.0865 - val_loss: 2.7197\n",
      "Epoch 360/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 57.5623 - val_loss: 2.7174\n",
      "Epoch 361/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 57.1155 - val_loss: 2.7466\n",
      "Epoch 362/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 39.7770 - val_loss: 2.7666\n",
      "Epoch 363/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 35.2369 - val_loss: 2.7468\n",
      "Epoch 364/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 56.1819 - val_loss: 2.8622\n",
      "Epoch 365/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 56.7806 - val_loss: 2.8287\n",
      "Epoch 366/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 65.4399 - val_loss: 2.8583\n",
      "Epoch 367/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 34.2931 - val_loss: 3.0565\n",
      "Epoch 368/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 47.8048 - val_loss: 3.3637\n",
      "Epoch 369/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 72.9317 - val_loss: 3.7686\n",
      "Epoch 370/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 51.6496 - val_loss: 4.3878\n",
      "Epoch 371/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 38.7593 - val_loss: 4.9234\n",
      "Epoch 372/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 35.5106 - val_loss: 5.5223\n",
      "Epoch 373/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 45.0490 - val_loss: 6.0295\n",
      "Epoch 374/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 62.3941 - val_loss: 6.5431\n",
      "Epoch 375/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 47.4752 - val_loss: 6.6280\n",
      "Epoch 376/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 37.3121 - val_loss: 6.8187\n",
      "Epoch 377/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 41.5674 - val_loss: 6.6825\n",
      "Epoch 378/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 53.9561 - val_loss: 6.4194\n",
      "Epoch 379/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 31.1177 - val_loss: 6.1400\n",
      "Epoch 380/2000\n",
      "81/81 [==============================] - 0s 121us/step - loss: 32.2973 - val_loss: 5.6421\n",
      "Epoch 381/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 61.5255 - val_loss: 5.1688\n",
      "Epoch 382/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 37.3565 - val_loss: 4.7602\n",
      "Epoch 383/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 35.0538 - val_loss: 4.4810\n",
      "Epoch 384/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 38.2393 - val_loss: 4.1438\n",
      "Epoch 385/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 49.3654 - val_loss: 4.0815\n",
      "Epoch 386/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 41.7403 - val_loss: 3.9589\n",
      "Epoch 387/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 60.8504 - val_loss: 3.7487\n",
      "Epoch 388/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 39.5539 - val_loss: 3.3388\n",
      "Epoch 389/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 29.0059 - val_loss: 2.9630\n",
      "Epoch 390/2000\n",
      "81/81 [==============================] - 0s 95us/step - loss: 33.7463 - val_loss: 2.8159\n",
      "Epoch 391/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 44.5424 - val_loss: 2.6437\n",
      "Epoch 392/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 32.1389 - val_loss: 2.6772\n",
      "Epoch 393/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 43.5526 - val_loss: 2.9022\n",
      "Epoch 394/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 28.8559 - val_loss: 3.0568\n",
      "Epoch 395/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 36.7415 - val_loss: 3.3143\n",
      "Epoch 396/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 55.4963 - val_loss: 3.6345\n",
      "Epoch 397/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 39.7896 - val_loss: 4.0958\n",
      "Epoch 398/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 41.2259 - val_loss: 4.4131\n",
      "Epoch 399/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 66.4754 - val_loss: 4.7602\n",
      "Epoch 400/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 43.4870 - val_loss: 4.8082\n",
      "Epoch 401/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 48.3497 - val_loss: 4.6329\n",
      "Epoch 402/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 45.0690 - val_loss: 4.4602\n",
      "Epoch 403/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 46.8165 - val_loss: 4.3593\n",
      "Epoch 404/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 61.9241 - val_loss: 3.9765\n",
      "Epoch 405/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 48.8455 - val_loss: 3.4557\n",
      "Epoch 406/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 42.0503 - val_loss: 3.1177\n",
      "Epoch 407/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 50.7242 - val_loss: 2.7510\n",
      "Epoch 408/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 38.9832 - val_loss: 2.5117\n",
      "Epoch 409/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 34.7587 - val_loss: 2.3367\n",
      "Epoch 410/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 45.3070 - val_loss: 2.1752\n",
      "Epoch 411/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 46.5170 - val_loss: 2.0784\n",
      "Epoch 412/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 44.5819 - val_loss: 1.8859\n",
      "Epoch 413/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 57.6332 - val_loss: 1.7754\n",
      "Epoch 414/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 41.8557 - val_loss: 1.7562\n",
      "Epoch 415/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 35.8484 - val_loss: 1.7317\n",
      "Epoch 416/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 41.1366 - val_loss: 1.8203\n",
      "Epoch 417/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 37.6987 - val_loss: 1.8479\n",
      "Epoch 418/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 39.3692 - val_loss: 1.6644\n",
      "Epoch 419/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 40.0787 - val_loss: 1.6533\n",
      "Epoch 420/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 33.7112 - val_loss: 1.6626\n",
      "Epoch 421/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 54.6900 - val_loss: 1.7679\n",
      "Epoch 422/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 41.3705 - val_loss: 1.8168\n",
      "Epoch 423/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 35.5024 - val_loss: 1.9451\n",
      "Epoch 424/2000\n",
      "81/81 [==============================] - 0s 108us/step - loss: 71.9369 - val_loss: 2.0915\n",
      "Epoch 425/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 47.5129 - val_loss: 2.2352\n",
      "Epoch 426/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 39.8005 - val_loss: 2.3859\n",
      "Epoch 427/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 36.2984 - val_loss: 2.4832\n",
      "Epoch 428/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 52.7520 - val_loss: 2.5514\n",
      "Epoch 429/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 33.4719 - val_loss: 2.8040\n",
      "Epoch 430/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 39.0849 - val_loss: 2.8247\n",
      "Epoch 431/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 57.4566 - val_loss: 2.8383\n",
      "Epoch 432/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 38.9034 - val_loss: 2.9608\n",
      "Epoch 433/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 37.1127 - val_loss: 3.1722\n",
      "Epoch 434/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 68.6119 - val_loss: 3.1818\n",
      "Epoch 435/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 35.2200 - val_loss: 3.0489\n",
      "Epoch 436/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 39.3375 - val_loss: 2.9823\n",
      "Epoch 437/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 44.6162 - val_loss: 3.0439\n",
      "Epoch 438/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 35.3069 - val_loss: 3.1195\n",
      "Epoch 439/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 51.1758 - val_loss: 3.0740\n",
      "Epoch 440/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 50.3117 - val_loss: 2.6768\n",
      "Epoch 441/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 40.7701 - val_loss: 2.5053\n",
      "Epoch 442/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 47.7820 - val_loss: 2.3677\n",
      "Epoch 443/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 32.4982 - val_loss: 2.1733\n",
      "Epoch 444/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 40.3481 - val_loss: 1.9766\n",
      "Epoch 445/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 32.5691 - val_loss: 1.8332\n",
      "Epoch 446/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 40.6348 - val_loss: 1.7155\n",
      "Epoch 447/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 39.4750 - val_loss: 1.6894\n",
      "Epoch 448/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 31.2777 - val_loss: 1.6172\n",
      "Epoch 449/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 25.6294 - val_loss: 1.6359\n",
      "Epoch 450/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 34.8122 - val_loss: 1.7054\n",
      "Epoch 451/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 55.9441 - val_loss: 1.7716\n",
      "Epoch 452/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 40.2895 - val_loss: 1.8501\n",
      "Epoch 453/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 27.5264 - val_loss: 2.0295\n",
      "Epoch 454/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 37.9313 - val_loss: 2.2255\n",
      "Epoch 455/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 42.6487 - val_loss: 2.1463\n",
      "Epoch 456/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 30.2762 - val_loss: 2.1494\n",
      "Epoch 457/2000\n",
      "81/81 [==============================] - 0s 106us/step - loss: 35.5194 - val_loss: 2.1506\n",
      "Epoch 458/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 38.2936 - val_loss: 2.0630\n",
      "Epoch 459/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 74.7199 - val_loss: 1.9803\n",
      "Epoch 460/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 33.9712 - val_loss: 1.9132\n",
      "Epoch 461/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 50.4628 - val_loss: 1.9024\n",
      "Epoch 462/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 37.7842 - val_loss: 1.8506\n",
      "Epoch 463/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 42.3766 - val_loss: 1.7681\n",
      "Epoch 464/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 48.0912 - val_loss: 1.7151\n",
      "Epoch 465/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 40.4094 - val_loss: 1.7445\n",
      "Epoch 466/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 32.3807 - val_loss: 1.7933\n",
      "Epoch 467/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 33.3124 - val_loss: 1.8458\n",
      "Epoch 468/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 24.6159 - val_loss: 1.8947\n",
      "Epoch 469/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 46.8806 - val_loss: 1.9318\n",
      "Epoch 470/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 18.5418 - val_loss: 1.9587\n",
      "Epoch 471/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 37.6904 - val_loss: 1.9775\n",
      "Epoch 472/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 38.7867 - val_loss: 2.1486\n",
      "Epoch 473/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 27.9639 - val_loss: 2.1420\n",
      "Epoch 474/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 39.5654 - val_loss: 2.0577\n",
      "Epoch 475/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 24.6280 - val_loss: 1.8857\n",
      "Epoch 476/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 37.5114 - val_loss: 1.8262\n",
      "Epoch 477/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 37.1606 - val_loss: 1.9716\n",
      "Epoch 478/2000\n",
      "81/81 [==============================] - 0s 110us/step - loss: 90.6238 - val_loss: 2.1058\n",
      "Epoch 479/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 50.0988 - val_loss: 2.0995\n",
      "Epoch 480/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 30.4372 - val_loss: 2.0643\n",
      "Epoch 481/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 36.4234 - val_loss: 2.0798\n",
      "Epoch 482/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 26.8613 - val_loss: 2.0576\n",
      "Epoch 483/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 24.4760 - val_loss: 2.0192\n",
      "Epoch 484/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 30.4144 - val_loss: 2.0080\n",
      "Epoch 485/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 26.1879 - val_loss: 1.9247\n",
      "Epoch 486/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 27.3779 - val_loss: 1.7981\n",
      "Epoch 487/2000\n",
      "81/81 [==============================] - 0s 120us/step - loss: 23.7185 - val_loss: 1.6527\n",
      "Epoch 488/2000\n",
      "81/81 [==============================] - 0s 291us/step - loss: 35.9228 - val_loss: 1.4771\n",
      "Epoch 489/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 29.2684 - val_loss: 1.3686\n",
      "Epoch 490/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 34.0225 - val_loss: 1.1454\n",
      "Epoch 491/2000\n",
      "81/81 [==============================] - 0s 112us/step - loss: 39.5723 - val_loss: 0.9967\n",
      "Epoch 492/2000\n",
      "81/81 [==============================] - 0s 108us/step - loss: 23.0207 - val_loss: 0.8747\n",
      "Epoch 493/2000\n",
      "81/81 [==============================] - 0s 118us/step - loss: 20.9601 - val_loss: 0.7819\n",
      "Epoch 494/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 33.8248 - val_loss: 0.7577\n",
      "Epoch 495/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 35.5359 - val_loss: 0.7421\n",
      "Epoch 496/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 36.3037 - val_loss: 0.8233\n",
      "Epoch 497/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 26.0769 - val_loss: 0.9200\n",
      "Epoch 498/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 29.9156 - val_loss: 1.0610\n",
      "Epoch 499/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 33.3181 - val_loss: 1.2801\n",
      "Epoch 500/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 38.9750 - val_loss: 1.5560\n",
      "Epoch 501/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 28.6020 - val_loss: 1.7470\n",
      "Epoch 502/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 22.8851 - val_loss: 1.9156\n",
      "Epoch 503/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 30.6016 - val_loss: 2.0563\n",
      "Epoch 504/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 40.9918 - val_loss: 2.0184\n",
      "Epoch 505/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 32.0621 - val_loss: 1.8607\n",
      "Epoch 506/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 19.5411 - val_loss: 1.6921\n",
      "Epoch 507/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 30.7557 - val_loss: 1.4848\n",
      "Epoch 508/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 30.8848 - val_loss: 1.2440\n",
      "Epoch 509/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 34.0270 - val_loss: 0.9419\n",
      "Epoch 510/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 20.7597 - val_loss: 0.7419\n",
      "Epoch 511/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 31.3064 - val_loss: 0.5436\n",
      "Epoch 512/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 30.9524 - val_loss: 0.4539\n",
      "Epoch 513/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 29.9457 - val_loss: 0.4320\n",
      "Epoch 514/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 14.3340 - val_loss: 0.4220\n",
      "Epoch 515/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 32.8384 - val_loss: 0.3697\n",
      "Epoch 516/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 21.4737 - val_loss: 0.3328\n",
      "Epoch 517/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 30.0290 - val_loss: 0.3291\n",
      "Epoch 518/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 25.9392 - val_loss: 0.3817\n",
      "Epoch 519/2000\n",
      "81/81 [==============================] - 0s 115us/step - loss: 32.4619 - val_loss: 0.4429\n",
      "Epoch 520/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 29.3437 - val_loss: 0.5558\n",
      "Epoch 521/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 46.6667 - val_loss: 0.7206\n",
      "Epoch 522/2000\n",
      "81/81 [==============================] - 0s 111us/step - loss: 21.0484 - val_loss: 0.9965\n",
      "Epoch 523/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 19.6181 - val_loss: 1.3774\n",
      "Epoch 524/2000\n",
      "81/81 [==============================] - 0s 116us/step - loss: 26.5217 - val_loss: 1.6930\n",
      "Epoch 525/2000\n",
      "81/81 [==============================] - 0s 113us/step - loss: 35.7583 - val_loss: 1.9541\n",
      "Epoch 526/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 36.2552 - val_loss: 2.1235\n",
      "Epoch 527/2000\n",
      "81/81 [==============================] - 0s 114us/step - loss: 31.8737 - val_loss: 2.3070\n",
      "Epoch 528/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 29.8496 - val_loss: 2.4685\n",
      "Epoch 529/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 22.1156 - val_loss: 2.4602\n",
      "Epoch 530/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 42.3303 - val_loss: 2.2701\n",
      "Epoch 531/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 17.5922 - val_loss: 1.9607\n",
      "Epoch 532/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 26.7623 - val_loss: 1.5942\n",
      "Epoch 533/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 23.6606 - val_loss: 1.3435\n",
      "Epoch 534/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 28.3502 - val_loss: 1.1109\n",
      "Epoch 535/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 46.8780 - val_loss: 0.7729\n",
      "Epoch 536/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 32.4264 - val_loss: 0.5571\n",
      "Epoch 537/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 17.0240 - val_loss: 0.4461\n",
      "Epoch 538/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 26.0038 - val_loss: 0.4133\n",
      "Epoch 539/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 18.9000 - val_loss: 0.4033\n",
      "Epoch 540/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 39.3149 - val_loss: 0.4495\n",
      "Epoch 541/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 26.5220 - val_loss: 0.5385\n",
      "Epoch 542/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 25.0215 - val_loss: 0.6813\n",
      "Epoch 543/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 38.6873 - val_loss: 0.7712\n",
      "Epoch 544/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 26.9763 - val_loss: 0.7647\n",
      "Epoch 545/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 21.9984 - val_loss: 0.6829\n",
      "Epoch 546/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 26.8125 - val_loss: 0.6276\n",
      "Epoch 547/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 25.1969 - val_loss: 0.6069\n",
      "Epoch 548/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 33.5240 - val_loss: 0.6907\n",
      "Epoch 549/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 36.1052 - val_loss: 0.7210\n",
      "Epoch 550/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 25.5539 - val_loss: 0.7454\n",
      "Epoch 551/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 49.6548 - val_loss: 0.7452\n",
      "Epoch 552/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 33.4970 - val_loss: 0.7343\n",
      "Epoch 553/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 28.8659 - val_loss: 0.6334\n",
      "Epoch 554/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 27.0844 - val_loss: 0.4935\n",
      "Epoch 555/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 22.5753 - val_loss: 0.4362\n",
      "Epoch 556/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 19.9232 - val_loss: 0.3859\n",
      "Epoch 557/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 17.4690 - val_loss: 0.3809\n",
      "Epoch 558/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 30.3841 - val_loss: 0.4141\n",
      "Epoch 559/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 39.3120 - val_loss: 0.4579\n",
      "Epoch 560/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 27.2392 - val_loss: 0.4834\n",
      "Epoch 561/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 25.8039 - val_loss: 0.5814\n",
      "Epoch 562/2000\n",
      "81/81 [==============================] - 0s 198us/step - loss: 30.5839 - val_loss: 0.6681\n",
      "Epoch 563/2000\n",
      "81/81 [==============================] - 0s 124us/step - loss: 27.9176 - val_loss: 0.6734\n",
      "Epoch 564/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 24.1488 - val_loss: 0.7136\n",
      "Epoch 565/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 28.3930 - val_loss: 0.7568\n",
      "Epoch 566/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 18.4766 - val_loss: 0.8377\n",
      "Epoch 567/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 14.2094 - val_loss: 0.8716\n",
      "Epoch 568/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 29.6090 - val_loss: 0.8732\n",
      "Epoch 569/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 24.6645 - val_loss: 0.8388\n",
      "Epoch 570/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 22.0040 - val_loss: 0.7628\n",
      "Epoch 571/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 19.9244 - val_loss: 0.7046\n",
      "Epoch 572/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 26.3109 - val_loss: 0.6478\n",
      "Epoch 573/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 20.2502 - val_loss: 0.5962\n",
      "Epoch 574/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 27.8022 - val_loss: 0.5644\n",
      "Epoch 575/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 30.0998 - val_loss: 0.5531\n",
      "Epoch 576/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 20.6416 - val_loss: 0.5649\n",
      "Epoch 577/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 21.8209 - val_loss: 0.5967\n",
      "Epoch 578/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 20.8018 - val_loss: 0.6180\n",
      "Epoch 579/2000\n",
      "81/81 [==============================] - 0s 124us/step - loss: 25.7800 - val_loss: 0.5961\n",
      "Epoch 580/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 25.2562 - val_loss: 0.5450\n",
      "Epoch 581/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 20.4068 - val_loss: 0.4952\n",
      "Epoch 582/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 20.5328 - val_loss: 0.4138\n",
      "Epoch 583/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 17.0719 - val_loss: 0.3059\n",
      "Epoch 584/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 14.3284 - val_loss: 0.2624\n",
      "Epoch 585/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 24.0057 - val_loss: 0.2268\n",
      "Epoch 586/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 19.3497 - val_loss: 0.1944\n",
      "Epoch 587/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 34.1253 - val_loss: 0.1924\n",
      "Epoch 588/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 29.7038 - val_loss: 0.2072\n",
      "Epoch 589/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 21.1157 - val_loss: 0.2830\n",
      "Epoch 590/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 24.9788 - val_loss: 0.3692\n",
      "Epoch 591/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 21.7063 - val_loss: 0.4501\n",
      "Epoch 592/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 30.2184 - val_loss: 0.5981\n",
      "Epoch 593/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 27.4920 - val_loss: 0.6959\n",
      "Epoch 594/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 22.2693 - val_loss: 0.7409\n",
      "Epoch 595/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 21.4693 - val_loss: 0.7104\n",
      "Epoch 596/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 28.4303 - val_loss: 0.6064\n",
      "Epoch 597/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 15.9642 - val_loss: 0.4608\n",
      "Epoch 598/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 24.2461 - val_loss: 0.4004\n",
      "Epoch 599/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 27.9935 - val_loss: 0.3398\n",
      "Epoch 600/2000\n",
      "81/81 [==============================] - 0s 51us/step - loss: 36.7799 - val_loss: 0.3322\n",
      "Epoch 601/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 19.8325 - val_loss: 0.3373\n",
      "Epoch 602/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 17.1334 - val_loss: 0.3319\n",
      "Epoch 603/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 27.8137 - val_loss: 0.2936\n",
      "Epoch 604/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 28.2319 - val_loss: 0.2609\n",
      "Epoch 605/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 22.3133 - val_loss: 0.2326\n",
      "Epoch 606/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 25.8947 - val_loss: 0.2463\n",
      "Epoch 607/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 19.4745 - val_loss: 0.2804\n",
      "Epoch 608/2000\n",
      "81/81 [==============================] - 0s 47us/step - loss: 23.6483 - val_loss: 0.3516\n",
      "Epoch 609/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 21.1845 - val_loss: 0.4257\n",
      "Epoch 610/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 30.0820 - val_loss: 0.4816\n",
      "Epoch 611/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 30.1391 - val_loss: 0.5097\n",
      "Epoch 612/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 16.0017 - val_loss: 0.5584\n",
      "Epoch 613/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 22.2955 - val_loss: 0.6326\n",
      "Epoch 614/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 49.6043 - val_loss: 0.7131\n",
      "Epoch 615/2000\n",
      "81/81 [==============================] - 0s 111us/step - loss: 15.2260 - val_loss: 0.7673\n",
      "Epoch 616/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 14.2709 - val_loss: 0.8024\n",
      "Epoch 617/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 24.4153 - val_loss: 0.8469\n",
      "Epoch 618/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 19.3513 - val_loss: 0.8624\n",
      "Epoch 619/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 24.6079 - val_loss: 0.7094\n",
      "Epoch 620/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 14.3896 - val_loss: 0.5238\n",
      "Epoch 621/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 22.2653 - val_loss: 0.3589\n",
      "Epoch 622/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 28.6966 - val_loss: 0.2305\n",
      "Epoch 623/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 22.5831 - val_loss: 0.1803\n",
      "Epoch 624/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 18.5735 - val_loss: 0.1614\n",
      "Epoch 625/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 17.3283 - val_loss: 0.1591\n",
      "Epoch 626/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 16.5342 - val_loss: 0.1599\n",
      "Epoch 627/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 33.3832 - val_loss: 0.1606\n",
      "Epoch 628/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 32.1548 - val_loss: 0.1664\n",
      "Epoch 629/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 28.3183 - val_loss: 0.1887\n",
      "Epoch 630/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 22.4237 - val_loss: 0.2174\n",
      "Epoch 631/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 20.0808 - val_loss: 0.2617\n",
      "Epoch 632/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 18.5842 - val_loss: 0.3108\n",
      "Epoch 633/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 27.4149 - val_loss: 0.3586\n",
      "Epoch 634/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 31.0077 - val_loss: 0.4578\n",
      "Epoch 635/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 16.4747 - val_loss: 0.6182\n",
      "Epoch 636/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 41.1707 - val_loss: 0.7949\n",
      "Epoch 637/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 19.7047 - val_loss: 0.9803\n",
      "Epoch 638/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 18.0208 - val_loss: 0.9516\n",
      "Epoch 639/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 15.9063 - val_loss: 0.9319\n",
      "Epoch 640/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 23.3055 - val_loss: 0.8567\n",
      "Epoch 641/2000\n",
      "81/81 [==============================] - 0s 95us/step - loss: 17.7408 - val_loss: 0.7929\n",
      "Epoch 642/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 22.1450 - val_loss: 0.7045\n",
      "Epoch 643/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 22.2138 - val_loss: 0.6943\n",
      "Epoch 644/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 18.5465 - val_loss: 0.5968\n",
      "Epoch 645/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 22.9384 - val_loss: 0.5260\n",
      "Epoch 646/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 20.4698 - val_loss: 0.4506\n",
      "Epoch 647/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 18.3766 - val_loss: 0.3935\n",
      "Epoch 648/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 18.7610 - val_loss: 0.3631\n",
      "Epoch 649/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 24.9426 - val_loss: 0.4360\n",
      "Epoch 650/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 19.6806 - val_loss: 0.5144\n",
      "Epoch 651/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 17.0350 - val_loss: 0.5568\n",
      "Epoch 652/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 19.9233 - val_loss: 0.5532\n",
      "Epoch 653/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 20.6714 - val_loss: 0.5752\n",
      "Epoch 654/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 28.3981 - val_loss: 0.5670\n",
      "Epoch 655/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 22.3363 - val_loss: 0.5598\n",
      "Epoch 656/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 18.6531 - val_loss: 0.4974\n",
      "Epoch 657/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 17.8878 - val_loss: 0.4360\n",
      "Epoch 658/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 28.3767 - val_loss: 0.4134\n",
      "Epoch 659/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 15.6270 - val_loss: 0.4538\n",
      "Epoch 660/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 34.9337 - val_loss: 0.4606\n",
      "Epoch 661/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 15.9456 - val_loss: 0.4839\n",
      "Epoch 662/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 11.3855 - val_loss: 0.4920\n",
      "Epoch 663/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 16.7950 - val_loss: 0.5208\n",
      "Epoch 664/2000\n",
      "81/81 [==============================] - 0s 102us/step - loss: 19.8470 - val_loss: 0.5634\n",
      "Epoch 665/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 26.2394 - val_loss: 0.5495\n",
      "Epoch 666/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 16.8042 - val_loss: 0.4917\n",
      "Epoch 667/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 62.4495 - val_loss: 0.3702\n",
      "Epoch 668/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 17.0241 - val_loss: 0.2884\n",
      "Epoch 669/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 24.6041 - val_loss: 0.2278\n",
      "Epoch 670/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 22.9760 - val_loss: 0.1866\n",
      "Epoch 671/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 23.1336 - val_loss: 0.1647\n",
      "Epoch 672/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 19.3216 - val_loss: 0.1591\n",
      "Epoch 673/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 15.3238 - val_loss: 0.1601\n",
      "Epoch 674/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 28.1063 - val_loss: 0.1615\n",
      "Epoch 675/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 17.8971 - val_loss: 0.1632\n",
      "Epoch 676/2000\n",
      "81/81 [==============================] - 0s 158us/step - loss: 22.0548 - val_loss: 0.1671\n",
      "Epoch 677/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 22.7482 - val_loss: 0.1633\n",
      "Epoch 678/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 14.9918 - val_loss: 0.1592\n",
      "Epoch 679/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 22.5543 - val_loss: 0.1640\n",
      "Epoch 680/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 20.9010 - val_loss: 0.2127\n",
      "Epoch 681/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 13.1559 - val_loss: 0.3363\n",
      "Epoch 682/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 21.4493 - val_loss: 0.5220\n",
      "Epoch 683/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 18.5352 - val_loss: 0.7334\n",
      "Epoch 684/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 31.1299 - val_loss: 0.9127\n",
      "Epoch 685/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 18.8367 - val_loss: 1.0384\n",
      "Epoch 686/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 14.9485 - val_loss: 1.0491\n",
      "Epoch 687/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 20.2288 - val_loss: 0.9218\n",
      "Epoch 688/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 22.1106 - val_loss: 0.6608\n",
      "Epoch 689/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 13.7111 - val_loss: 0.4866\n",
      "Epoch 690/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 19.2952 - val_loss: 0.3540\n",
      "Epoch 691/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 14.6548 - val_loss: 0.2612\n",
      "Epoch 692/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 28.1982 - val_loss: 0.2133\n",
      "Epoch 693/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 21.7471 - val_loss: 0.1998\n",
      "Epoch 694/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 18.2771 - val_loss: 0.1938\n",
      "Epoch 695/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 27.6796 - val_loss: 0.1940\n",
      "Epoch 696/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 23.3520 - val_loss: 0.2007\n",
      "Epoch 697/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 15.3058 - val_loss: 0.2173\n",
      "Epoch 698/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 25.4153 - val_loss: 0.2120\n",
      "Epoch 699/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 15.4041 - val_loss: 0.2248\n",
      "Epoch 700/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 22.3770 - val_loss: 0.2591\n",
      "Epoch 701/2000\n",
      "81/81 [==============================] - 0s 50us/step - loss: 17.4449 - val_loss: 0.3389\n",
      "Epoch 702/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 27.1096 - val_loss: 0.4358\n",
      "Epoch 703/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 23.6160 - val_loss: 0.5692\n",
      "Epoch 704/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 19.0037 - val_loss: 0.7035\n",
      "Epoch 705/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 16.9400 - val_loss: 0.7826\n",
      "Epoch 706/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 11.6456 - val_loss: 0.7709\n",
      "Epoch 707/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 19.0009 - val_loss: 0.7066\n",
      "Epoch 708/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 18.2437 - val_loss: 0.7177\n",
      "Epoch 709/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 17.9587 - val_loss: 0.7310\n",
      "Epoch 710/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 17.1831 - val_loss: 0.7274\n",
      "Epoch 711/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 19.2241 - val_loss: 0.6528\n",
      "Epoch 712/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 47.0762 - val_loss: 0.4334\n",
      "Epoch 713/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 29.2610 - val_loss: 0.2713\n",
      "Epoch 714/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 32.5872 - val_loss: 0.1979\n",
      "Epoch 715/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 27.0046 - val_loss: 0.1847\n",
      "Epoch 716/2000\n",
      "81/81 [==============================] - 0s 50us/step - loss: 18.6993 - val_loss: 0.1760\n",
      "Epoch 717/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 14.5214 - val_loss: 0.1675\n",
      "Epoch 718/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 18.8763 - val_loss: 0.1662\n",
      "Epoch 719/2000\n",
      "81/81 [==============================] - 0s 50us/step - loss: 15.3992 - val_loss: 0.1623\n",
      "Epoch 720/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 16.0984 - val_loss: 0.1645\n",
      "Epoch 721/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 14.6771 - val_loss: 0.1726\n",
      "Epoch 722/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 13.5857 - val_loss: 0.2017\n",
      "Epoch 723/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 30.2393 - val_loss: 0.2589\n",
      "Epoch 724/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 14.7828 - val_loss: 0.3328\n",
      "Epoch 725/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 15.2057 - val_loss: 0.4105\n",
      "Epoch 726/2000\n",
      "81/81 [==============================] - 0s 135us/step - loss: 21.1507 - val_loss: 0.4728\n",
      "Epoch 727/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 20.0047 - val_loss: 0.5180\n",
      "Epoch 728/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 13.1023 - val_loss: 0.4907\n",
      "Epoch 729/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 32.6200 - val_loss: 0.4555\n",
      "Epoch 730/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 25.0543 - val_loss: 0.4095\n",
      "Epoch 731/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 17.2705 - val_loss: 0.3980\n",
      "Epoch 732/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 11.9269 - val_loss: 0.3893\n",
      "Epoch 733/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 19.2459 - val_loss: 0.2962\n",
      "Epoch 734/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 20.4691 - val_loss: 0.2366\n",
      "Epoch 735/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 31.7239 - val_loss: 0.1919\n",
      "Epoch 736/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 11.8688 - val_loss: 0.1734\n",
      "Epoch 737/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 16.8374 - val_loss: 0.1676\n",
      "Epoch 738/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 21.4990 - val_loss: 0.1614\n",
      "Epoch 739/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 16.0704 - val_loss: 0.1593\n",
      "Epoch 740/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 22.5045 - val_loss: 0.1612\n",
      "Epoch 741/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 12.2059 - val_loss: 0.1641\n",
      "Epoch 742/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 27.4528 - val_loss: 0.1600\n",
      "Epoch 743/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 12.5504 - val_loss: 0.1640\n",
      "Epoch 744/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 22.8165 - val_loss: 0.1774\n",
      "Epoch 745/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 10.6465 - val_loss: 0.1881\n",
      "Epoch 746/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 32.8163 - val_loss: 0.1884\n",
      "Epoch 747/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 24.4569 - val_loss: 0.1887\n",
      "Epoch 748/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 14.9178 - val_loss: 0.1860\n",
      "Epoch 749/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 14.9251 - val_loss: 0.1850\n",
      "Epoch 750/2000\n",
      "81/81 [==============================] - 0s 49us/step - loss: 13.5952 - val_loss: 0.1823\n",
      "Epoch 751/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 24.3480 - val_loss: 0.1858\n",
      "Epoch 752/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 21.5058 - val_loss: 0.1952\n",
      "Epoch 753/2000\n",
      "81/81 [==============================] - 0s 50us/step - loss: 12.1949 - val_loss: 0.2002\n",
      "Epoch 754/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 20.8669 - val_loss: 0.1784\n",
      "Epoch 755/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 9.0967 - val_loss: 0.1796\n",
      "Epoch 756/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 16.2635 - val_loss: 0.1773\n",
      "Epoch 757/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 10.8332 - val_loss: 0.1751\n",
      "Epoch 758/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 11.0986 - val_loss: 0.1765\n",
      "Epoch 759/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 14.7818 - val_loss: 0.1889\n",
      "Epoch 760/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 20.2156 - val_loss: 0.1999\n",
      "Epoch 761/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 12.5313 - val_loss: 0.1991\n",
      "Epoch 762/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 14.1547 - val_loss: 0.2165\n",
      "Epoch 763/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 15.0354 - val_loss: 0.2771\n",
      "Epoch 764/2000\n",
      "81/81 [==============================] - 0s 108us/step - loss: 7.8499 - val_loss: 0.3801\n",
      "Epoch 765/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 18.0321 - val_loss: 0.4633\n",
      "Epoch 766/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 18.2909 - val_loss: 0.4865\n",
      "Epoch 767/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 11.7480 - val_loss: 0.4780\n",
      "Epoch 768/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 13.8118 - val_loss: 0.5216\n",
      "Epoch 769/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 12.8174 - val_loss: 0.5404\n",
      "Epoch 770/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 15.1534 - val_loss: 0.5298\n",
      "Epoch 771/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 9.5120 - val_loss: 0.5212\n",
      "Epoch 772/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 19.3386 - val_loss: 0.4164\n",
      "Epoch 773/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 12.7369 - val_loss: 0.3088\n",
      "Epoch 774/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 25.0186 - val_loss: 0.2272\n",
      "Epoch 775/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 21.3974 - val_loss: 0.1732\n",
      "Epoch 776/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 14.5908 - val_loss: 0.1597\n",
      "Epoch 777/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 12.1652 - val_loss: 0.1616\n",
      "Epoch 778/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 17.3149 - val_loss: 0.1672\n",
      "Epoch 779/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 17.6956 - val_loss: 0.1615\n",
      "Epoch 780/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 12.9904 - val_loss: 0.1610\n",
      "Epoch 781/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 15.1341 - val_loss: 0.2165\n",
      "Epoch 782/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 19.1637 - val_loss: 0.3799\n",
      "Epoch 783/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 13.6471 - val_loss: 0.6554\n",
      "Epoch 784/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 13.7878 - val_loss: 0.9927\n",
      "Epoch 785/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 12.1087 - val_loss: 1.2358\n",
      "Epoch 786/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 15.0224 - val_loss: 1.3223\n",
      "Epoch 787/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 14.9996 - val_loss: 1.3559\n",
      "Epoch 788/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 14.2180 - val_loss: 1.2750\n",
      "Epoch 789/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 14.6544 - val_loss: 1.2054\n",
      "Epoch 790/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 9.6957 - val_loss: 0.9577\n",
      "Epoch 791/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 14.0126 - val_loss: 0.7886\n",
      "Epoch 792/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 17.9401 - val_loss: 0.6048\n",
      "Epoch 793/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 11.2848 - val_loss: 0.4177\n",
      "Epoch 794/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 21.0413 - val_loss: 0.2563\n",
      "Epoch 795/2000\n",
      "81/81 [==============================] - 0s 117us/step - loss: 13.9518 - val_loss: 0.1745\n",
      "Epoch 796/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 17.2558 - val_loss: 0.1594\n",
      "Epoch 797/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 12.1518 - val_loss: 0.1627\n",
      "Epoch 798/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 15.5895 - val_loss: 0.1660\n",
      "Epoch 799/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 14.6322 - val_loss: 0.1666\n",
      "Epoch 800/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 9.9494 - val_loss: 0.1599\n",
      "Epoch 801/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 12.4300 - val_loss: 0.1634\n",
      "Epoch 802/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 20.3056 - val_loss: 0.1810\n",
      "Epoch 803/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 8.7296 - val_loss: 0.2039\n",
      "Epoch 804/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 11.6906 - val_loss: 0.2113\n",
      "Epoch 805/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 11.0876 - val_loss: 0.2252\n",
      "Epoch 806/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 15.1546 - val_loss: 0.2743\n",
      "Epoch 807/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 17.0846 - val_loss: 0.3065\n",
      "Epoch 808/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 14.4871 - val_loss: 0.3224\n",
      "Epoch 809/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 17.5563 - val_loss: 0.3221\n",
      "Epoch 810/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 20.3956 - val_loss: 0.2842\n",
      "Epoch 811/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 19.8624 - val_loss: 0.2322\n",
      "Epoch 812/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 10.4077 - val_loss: 0.2042\n",
      "Epoch 813/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 15.0419 - val_loss: 0.1922\n",
      "Epoch 814/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 16.8943 - val_loss: 0.1839\n",
      "Epoch 815/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 11.5870 - val_loss: 0.1658\n",
      "Epoch 816/2000\n",
      "81/81 [==============================] - 0s 48us/step - loss: 15.3440 - val_loss: 0.1592\n",
      "Epoch 817/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 13.2960 - val_loss: 0.1591\n",
      "Epoch 818/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 13.6612 - val_loss: 0.1600\n",
      "Epoch 819/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 25.2317 - val_loss: 0.1599\n",
      "Epoch 820/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 17.4015 - val_loss: 0.1592\n",
      "Epoch 821/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 12.3566 - val_loss: 0.1595\n",
      "Epoch 822/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 19.7106 - val_loss: 0.1724\n",
      "Epoch 823/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 15.7431 - val_loss: 0.2166\n",
      "Epoch 824/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 20.3221 - val_loss: 0.2923\n",
      "Epoch 825/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 10.5905 - val_loss: 0.4140\n",
      "Epoch 826/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 17.4417 - val_loss: 0.5337\n",
      "Epoch 827/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 11.9543 - val_loss: 0.6262\n",
      "Epoch 828/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 13.9480 - val_loss: 0.7142\n",
      "Epoch 829/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 13.5295 - val_loss: 0.7317\n",
      "Epoch 830/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 18.4322 - val_loss: 0.6313\n",
      "Epoch 831/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 12.8213 - val_loss: 0.4442\n",
      "Epoch 832/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 14.3863 - val_loss: 0.2651\n",
      "Epoch 833/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 12.0168 - val_loss: 0.2038\n",
      "Epoch 834/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 15.2195 - val_loss: 0.1967\n",
      "Epoch 835/2000\n",
      "81/81 [==============================] - 0s 121us/step - loss: 22.0197 - val_loss: 0.1892\n",
      "Epoch 836/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 20.7834 - val_loss: 0.1850\n",
      "Epoch 837/2000\n",
      "81/81 [==============================] - 0s 112us/step - loss: 11.0613 - val_loss: 0.1871\n",
      "Epoch 838/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 24.8262 - val_loss: 0.1863\n",
      "Epoch 839/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 14.1688 - val_loss: 0.2093\n",
      "Epoch 840/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 18.4868 - val_loss: 0.2455\n",
      "Epoch 841/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 8.2537 - val_loss: 0.3137\n",
      "Epoch 842/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 13.5344 - val_loss: 0.3674\n",
      "Epoch 843/2000\n",
      "81/81 [==============================] - 0s 153us/step - loss: 13.8473 - val_loss: 0.4173\n",
      "Epoch 844/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 14.1697 - val_loss: 0.4404\n",
      "Epoch 845/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 13.6321 - val_loss: 0.4870\n",
      "Epoch 846/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 13.0908 - val_loss: 0.5458\n",
      "Epoch 847/2000\n",
      "81/81 [==============================] - 0s 120us/step - loss: 9.7911 - val_loss: 0.5464\n",
      "Epoch 848/2000\n",
      "81/81 [==============================] - 0s 116us/step - loss: 13.2333 - val_loss: 0.4566\n",
      "Epoch 849/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 7.0813 - val_loss: 0.3539\n",
      "Epoch 850/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 9.1864 - val_loss: 0.2629\n",
      "Epoch 851/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 17.9880 - val_loss: 0.2208\n",
      "Epoch 852/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 13.8654 - val_loss: 0.2140\n",
      "Epoch 853/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 8.2217 - val_loss: 0.2121\n",
      "Epoch 854/2000\n",
      "81/81 [==============================] - 0s 50us/step - loss: 11.1606 - val_loss: 0.2185\n",
      "Epoch 855/2000\n",
      "81/81 [==============================] - 0s 140us/step - loss: 8.9671 - val_loss: 0.2122\n",
      "Epoch 856/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 10.8081 - val_loss: 0.2294\n",
      "Epoch 857/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 27.0132 - val_loss: 0.2689\n",
      "Epoch 858/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 9.2751 - val_loss: 0.3483\n",
      "Epoch 859/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 11.3351 - val_loss: 0.4751\n",
      "Epoch 860/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 14.8250 - val_loss: 0.6196\n",
      "Epoch 861/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 20.1000 - val_loss: 0.7905\n",
      "Epoch 862/2000\n",
      "81/81 [==============================] - 0s 103us/step - loss: 17.7565 - val_loss: 0.9033\n",
      "Epoch 863/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 18.5279 - val_loss: 0.8899\n",
      "Epoch 864/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 13.8070 - val_loss: 0.7175\n",
      "Epoch 865/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 10.8143 - val_loss: 0.5540\n",
      "Epoch 866/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 9.2066 - val_loss: 0.3928\n",
      "Epoch 867/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 11.1905 - val_loss: 0.3131\n",
      "Epoch 868/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 12.2568 - val_loss: 0.2749\n",
      "Epoch 869/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 22.3870 - val_loss: 0.2543\n",
      "Epoch 870/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 13.1131 - val_loss: 0.2385\n",
      "Epoch 871/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 9.0095 - val_loss: 0.2015\n",
      "Epoch 872/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 9.1147 - val_loss: 0.1922\n",
      "Epoch 873/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 14.7084 - val_loss: 0.1838\n",
      "Epoch 874/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 10.7530 - val_loss: 0.1818\n",
      "Epoch 875/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 29.6334 - val_loss: 0.1796\n",
      "Epoch 876/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 15.1753 - val_loss: 0.1725\n",
      "Epoch 877/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 12.6696 - val_loss: 0.1882\n",
      "Epoch 878/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 11.8296 - val_loss: 0.2097\n",
      "Epoch 879/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 12.3678 - val_loss: 0.2598\n",
      "Epoch 880/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 10.6489 - val_loss: 0.3158\n",
      "Epoch 881/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 15.1699 - val_loss: 0.3717\n",
      "Epoch 882/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 14.3648 - val_loss: 0.4530\n",
      "Epoch 883/2000\n",
      "81/81 [==============================] - 0s 113us/step - loss: 12.0978 - val_loss: 0.5873\n",
      "Epoch 884/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 14.9246 - val_loss: 0.6675\n",
      "Epoch 885/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 9.7238 - val_loss: 0.6841\n",
      "Epoch 886/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 12.3141 - val_loss: 0.5835\n",
      "Epoch 887/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 10.7623 - val_loss: 0.4808\n",
      "Epoch 888/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 14.8933 - val_loss: 0.3488\n",
      "Epoch 889/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 8.3152 - val_loss: 0.2695\n",
      "Epoch 890/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 13.3647 - val_loss: 0.2226\n",
      "Epoch 891/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 7.2079 - val_loss: 0.1980\n",
      "Epoch 892/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 11.8889 - val_loss: 0.1888\n",
      "Epoch 893/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 9.2172 - val_loss: 0.1958\n",
      "Epoch 894/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 10.0205 - val_loss: 0.2305\n",
      "Epoch 895/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 15.4117 - val_loss: 0.2463\n",
      "Epoch 896/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 13.4839 - val_loss: 0.2468\n",
      "Epoch 897/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 9.0721 - val_loss: 0.2839\n",
      "Epoch 898/2000\n",
      "81/81 [==============================] - 0s 102us/step - loss: 8.4657 - val_loss: 0.3003\n",
      "Epoch 899/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 18.4470 - val_loss: 0.3468\n",
      "Epoch 900/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 8.1568 - val_loss: 0.3903\n",
      "Epoch 901/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 8.6007 - val_loss: 0.4102\n",
      "Epoch 902/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 7.4648 - val_loss: 0.3978\n",
      "Epoch 903/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 14.3492 - val_loss: 0.3421\n",
      "Epoch 904/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 12.9315 - val_loss: 0.3017\n",
      "Epoch 905/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 12.0996 - val_loss: 0.2841\n",
      "Epoch 906/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 9.1068 - val_loss: 0.2602\n",
      "Epoch 907/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 14.7589 - val_loss: 0.2374\n",
      "Epoch 908/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 12.5483 - val_loss: 0.2545\n",
      "Epoch 909/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 11.2852 - val_loss: 0.3063\n",
      "Epoch 910/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 11.3564 - val_loss: 0.3400\n",
      "Epoch 911/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 12.2351 - val_loss: 0.4220\n",
      "Epoch 912/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 10.6438 - val_loss: 0.5452\n",
      "Epoch 913/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 7.9734 - val_loss: 0.6245\n",
      "Epoch 914/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 15.6319 - val_loss: 0.6310\n",
      "Epoch 915/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 8.9668 - val_loss: 0.6010\n",
      "Epoch 916/2000\n",
      "81/81 [==============================] - 0s 49us/step - loss: 9.2430 - val_loss: 0.4575\n",
      "Epoch 917/2000\n",
      "81/81 [==============================] - 0s 143us/step - loss: 13.9225 - val_loss: 0.3406\n",
      "Epoch 918/2000\n",
      "81/81 [==============================] - 0s 157us/step - loss: 15.5231 - val_loss: 0.2648\n",
      "Epoch 919/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 13.6539 - val_loss: 0.2540\n",
      "Epoch 920/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 13.3946 - val_loss: 0.2160\n",
      "Epoch 921/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 14.2345 - val_loss: 0.1782\n",
      "Epoch 922/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 15.8072 - val_loss: 0.1780\n",
      "Epoch 923/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 15.7077 - val_loss: 0.2205\n",
      "Epoch 924/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 8.3432 - val_loss: 0.3506\n",
      "Epoch 925/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 15.0012 - val_loss: 0.4542\n",
      "Epoch 926/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 15.4730 - val_loss: 0.5378\n",
      "Epoch 927/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 12.9495 - val_loss: 0.5717\n",
      "Epoch 928/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 13.4443 - val_loss: 0.5159\n",
      "Epoch 929/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 8.8283 - val_loss: 0.4315\n",
      "Epoch 930/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 12.6472 - val_loss: 0.3220\n",
      "Epoch 931/2000\n",
      "81/81 [==============================] - 0s 122us/step - loss: 8.9034 - val_loss: 0.2625\n",
      "Epoch 932/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 12.6674 - val_loss: 0.1913\n",
      "Epoch 933/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 11.1114 - val_loss: 0.1639\n",
      "Epoch 934/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 9.6667 - val_loss: 0.1692\n",
      "Epoch 935/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 10.3744 - val_loss: 0.1975\n",
      "Epoch 936/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 7.7294 - val_loss: 0.2589\n",
      "Epoch 937/2000\n",
      "81/81 [==============================] - 0s 49us/step - loss: 10.6710 - val_loss: 0.3181\n",
      "Epoch 938/2000\n",
      "81/81 [==============================] - 0s 51us/step - loss: 11.4837 - val_loss: 0.3873\n",
      "Epoch 939/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 7.4853 - val_loss: 0.4402\n",
      "Epoch 940/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 14.3602 - val_loss: 0.4924\n",
      "Epoch 941/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 11.3271 - val_loss: 0.5506\n",
      "Epoch 942/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 14.5647 - val_loss: 0.6430\n",
      "Epoch 943/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 18.3682 - val_loss: 0.6344\n",
      "Epoch 944/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 6.7811 - val_loss: 0.6616\n",
      "Epoch 945/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 9.4904 - val_loss: 0.6549\n",
      "Epoch 946/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 13.1514 - val_loss: 0.6229\n",
      "Epoch 947/2000\n",
      "81/81 [==============================] - 0s 48us/step - loss: 6.9653 - val_loss: 0.5735\n",
      "Epoch 948/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 8.3584 - val_loss: 0.4546\n",
      "Epoch 949/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 11.0465 - val_loss: 0.3736\n",
      "Epoch 950/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 5.7528 - val_loss: 0.3164\n",
      "Epoch 951/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 6.1807 - val_loss: 0.2536\n",
      "Epoch 952/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 13.7604 - val_loss: 0.1982\n",
      "Epoch 953/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 10.3440 - val_loss: 0.1889\n",
      "Epoch 954/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 6.3662 - val_loss: 0.1838\n",
      "Epoch 955/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 10.8902 - val_loss: 0.1942\n",
      "Epoch 956/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 10.5321 - val_loss: 0.2028\n",
      "Epoch 957/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 8.7225 - val_loss: 0.2159\n",
      "Epoch 958/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 13.5408 - val_loss: 0.2447\n",
      "Epoch 959/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 11.3836 - val_loss: 0.2558\n",
      "Epoch 960/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 10.1750 - val_loss: 0.2729\n",
      "Epoch 961/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 11.1471 - val_loss: 0.3119\n",
      "Epoch 962/2000\n",
      "81/81 [==============================] - 0s 135us/step - loss: 13.3892 - val_loss: 0.3908\n",
      "Epoch 963/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 7.4196 - val_loss: 0.4594\n",
      "Epoch 964/2000\n",
      "81/81 [==============================] - 0s 126us/step - loss: 9.0405 - val_loss: 0.5248\n",
      "Epoch 965/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 8.1343 - val_loss: 0.5867\n",
      "Epoch 966/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 12.4991 - val_loss: 0.6215\n",
      "Epoch 967/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 16.4123 - val_loss: 0.5371\n",
      "Epoch 968/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 10.3432 - val_loss: 0.4627\n",
      "Epoch 969/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 11.8610 - val_loss: 0.4198\n",
      "Epoch 970/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 9.1575 - val_loss: 0.3613\n",
      "Epoch 971/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 7.8907 - val_loss: 0.3231\n",
      "Epoch 972/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 10.3891 - val_loss: 0.3093\n",
      "Epoch 973/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 12.3132 - val_loss: 0.3456\n",
      "Epoch 974/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 7.1677 - val_loss: 0.3608\n",
      "Epoch 975/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 8.0117 - val_loss: 0.3962\n",
      "Epoch 976/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 11.1555 - val_loss: 0.4818\n",
      "Epoch 977/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 7.4329 - val_loss: 0.5941\n",
      "Epoch 978/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 7.7614 - val_loss: 0.6244\n",
      "Epoch 979/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 11.5842 - val_loss: 0.5787\n",
      "Epoch 980/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 6.9038 - val_loss: 0.5344\n",
      "Epoch 981/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 9.1605 - val_loss: 0.5006\n",
      "Epoch 982/2000\n",
      "81/81 [==============================] - 0s 51us/step - loss: 9.3372 - val_loss: 0.4411\n",
      "Epoch 983/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 8.4557 - val_loss: 0.3746\n",
      "Epoch 984/2000\n",
      "81/81 [==============================] - 0s 47us/step - loss: 20.8875 - val_loss: 0.3229\n",
      "Epoch 985/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 6.3344 - val_loss: 0.2829\n",
      "Epoch 986/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 7.3119 - val_loss: 0.2455\n",
      "Epoch 987/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 6.2131 - val_loss: 0.2231\n",
      "Epoch 988/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 12.7072 - val_loss: 0.2038\n",
      "Epoch 989/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 14.0750 - val_loss: 0.1929\n",
      "Epoch 990/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 6.2764 - val_loss: 0.1902\n",
      "Epoch 991/2000\n",
      "81/81 [==============================] - 0s 105us/step - loss: 8.2864 - val_loss: 0.2065\n",
      "Epoch 992/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 5.9890 - val_loss: 0.2324\n",
      "Epoch 993/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 9.2073 - val_loss: 0.2561\n",
      "Epoch 994/2000\n",
      "81/81 [==============================] - 0s 112us/step - loss: 10.0528 - val_loss: 0.2971\n",
      "Epoch 995/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 7.7875 - val_loss: 0.3341\n",
      "Epoch 996/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 6.8892 - val_loss: 0.3627\n",
      "Epoch 997/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 10.7114 - val_loss: 0.4031\n",
      "Epoch 998/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 12.4721 - val_loss: 0.4308\n",
      "Epoch 999/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 14.1775 - val_loss: 0.4698\n",
      "Epoch 1000/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 9.4153 - val_loss: 0.5686\n",
      "Epoch 1001/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 10.8747 - val_loss: 0.7042\n",
      "Epoch 1002/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 12.6406 - val_loss: 0.7459\n",
      "Epoch 1003/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 13.8834 - val_loss: 0.8080\n",
      "Epoch 1004/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 10.9317 - val_loss: 0.8856\n",
      "Epoch 1005/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 10.9886 - val_loss: 0.7856\n",
      "Epoch 1006/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 7.7681 - val_loss: 0.6484\n",
      "Epoch 1007/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 10.2349 - val_loss: 0.5418\n",
      "Epoch 1008/2000\n",
      "81/81 [==============================] - 0s 127us/step - loss: 8.9944 - val_loss: 0.4329\n",
      "Epoch 1009/2000\n",
      "81/81 [==============================] - 0s 150us/step - loss: 9.7092 - val_loss: 0.3297\n",
      "Epoch 1010/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 8.7207 - val_loss: 0.2612\n",
      "Epoch 1011/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 6.1405 - val_loss: 0.2156\n",
      "Epoch 1012/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 9.4230 - val_loss: 0.2038\n",
      "Epoch 1013/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.0829 - val_loss: 0.1975\n",
      "Epoch 1014/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 12.7441 - val_loss: 0.2272\n",
      "Epoch 1015/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 11.0421 - val_loss: 0.2651\n",
      "Epoch 1016/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 10.8182 - val_loss: 0.3190\n",
      "Epoch 1017/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 7.0819 - val_loss: 0.3722\n",
      "Epoch 1018/2000\n",
      "81/81 [==============================] - 0s 135us/step - loss: 12.3886 - val_loss: 0.4483\n",
      "Epoch 1019/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 7.9357 - val_loss: 0.5318\n",
      "Epoch 1020/2000\n",
      "81/81 [==============================] - 0s 103us/step - loss: 9.3191 - val_loss: 0.6965\n",
      "Epoch 1021/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 6.1380 - val_loss: 0.8584\n",
      "Epoch 1022/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 7.0133 - val_loss: 0.9582\n",
      "Epoch 1023/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 10.1912 - val_loss: 1.0381\n",
      "Epoch 1024/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 8.9973 - val_loss: 0.8886\n",
      "Epoch 1025/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 7.0995 - val_loss: 0.6021\n",
      "Epoch 1026/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 8.1373 - val_loss: 0.3990\n",
      "Epoch 1027/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 7.3950 - val_loss: 0.2853\n",
      "Epoch 1028/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 5.6630 - val_loss: 0.2059\n",
      "Epoch 1029/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 13.0350 - val_loss: 0.1809\n",
      "Epoch 1030/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 8.4173 - val_loss: 0.1824\n",
      "Epoch 1031/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 10.4841 - val_loss: 0.1865\n",
      "Epoch 1032/2000\n",
      "81/81 [==============================] - 0s 116us/step - loss: 10.1041 - val_loss: 0.1871\n",
      "Epoch 1033/2000\n",
      "81/81 [==============================] - 0s 107us/step - loss: 9.9197 - val_loss: 0.1968\n",
      "Epoch 1034/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 8.0589 - val_loss: 0.2522\n",
      "Epoch 1035/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 7.6586 - val_loss: 0.3742\n",
      "Epoch 1036/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 12.5925 - val_loss: 0.4932\n",
      "Epoch 1037/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 11.4483 - val_loss: 0.5993\n",
      "Epoch 1038/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 12.0514 - val_loss: 0.6911\n",
      "Epoch 1039/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 14.1750 - val_loss: 0.6992\n",
      "Epoch 1040/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 12.6784 - val_loss: 0.6125\n",
      "Epoch 1041/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 12.5185 - val_loss: 0.5252\n",
      "Epoch 1042/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 8.3101 - val_loss: 0.4612\n",
      "Epoch 1043/2000\n",
      "81/81 [==============================] - 0s 50us/step - loss: 12.0417 - val_loss: 0.3739\n",
      "Epoch 1044/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 15.2228 - val_loss: 0.3140\n",
      "Epoch 1045/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 6.7809 - val_loss: 0.2458\n",
      "Epoch 1046/2000\n",
      "81/81 [==============================] - 0s 134us/step - loss: 6.5380 - val_loss: 0.2191\n",
      "Epoch 1047/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 11.9800 - val_loss: 0.2315\n",
      "Epoch 1048/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 7.3091 - val_loss: 0.2527\n",
      "Epoch 1049/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 9.6765 - val_loss: 0.2684\n",
      "Epoch 1050/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 8.9916 - val_loss: 0.3097\n",
      "Epoch 1051/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 5.9978 - val_loss: 0.3729\n",
      "Epoch 1052/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 7.3722 - val_loss: 0.4987\n",
      "Epoch 1053/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 8.0592 - val_loss: 0.5890\n",
      "Epoch 1054/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 7.7224 - val_loss: 0.7054\n",
      "Epoch 1055/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 13.4550 - val_loss: 0.8077\n",
      "Epoch 1056/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 8.2182 - val_loss: 0.7886\n",
      "Epoch 1057/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 9.4433 - val_loss: 0.6800\n",
      "Epoch 1058/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 10.9921 - val_loss: 0.5296\n",
      "Epoch 1059/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 5.7864 - val_loss: 0.4138\n",
      "Epoch 1060/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 3.8088 - val_loss: 0.3344\n",
      "Epoch 1061/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 7.1248 - val_loss: 0.3149\n",
      "Epoch 1062/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 5.8526 - val_loss: 0.3269\n",
      "Epoch 1063/2000\n",
      "81/81 [==============================] - 0s 48us/step - loss: 7.9631 - val_loss: 0.3853\n",
      "Epoch 1064/2000\n",
      "81/81 [==============================] - 0s 51us/step - loss: 4.6756 - val_loss: 0.4857\n",
      "Epoch 1065/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 12.4952 - val_loss: 0.6197\n",
      "Epoch 1066/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 8.6968 - val_loss: 0.6573\n",
      "Epoch 1067/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 7.3684 - val_loss: 0.6489\n",
      "Epoch 1068/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 9.5014 - val_loss: 0.5439\n",
      "Epoch 1069/2000\n",
      "81/81 [==============================] - 0s 154us/step - loss: 5.6485 - val_loss: 0.4069\n",
      "Epoch 1070/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 14.3784 - val_loss: 0.3306\n",
      "Epoch 1071/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 8.5948 - val_loss: 0.2956\n",
      "Epoch 1072/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 7.9310 - val_loss: 0.2953\n",
      "Epoch 1073/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 18.2297 - val_loss: 0.3691\n",
      "Epoch 1074/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 10.2134 - val_loss: 0.4743\n",
      "Epoch 1075/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 14.3369 - val_loss: 0.5378\n",
      "Epoch 1076/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 19.6671 - val_loss: 0.6357\n",
      "Epoch 1077/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 5.3787 - val_loss: 0.7096\n",
      "Epoch 1078/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 10.4866 - val_loss: 0.7667\n",
      "Epoch 1079/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 10.5120 - val_loss: 0.7966\n",
      "Epoch 1080/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 17.8285 - val_loss: 0.8136\n",
      "Epoch 1081/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 8.9249 - val_loss: 0.6993\n",
      "Epoch 1082/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 16.5353 - val_loss: 0.4659\n",
      "Epoch 1083/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 6.2109 - val_loss: 0.2911\n",
      "Epoch 1084/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 4.8393 - val_loss: 0.2033\n",
      "Epoch 1085/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 9.5872 - val_loss: 0.1832\n",
      "Epoch 1086/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 6.0886 - val_loss: 0.2006\n",
      "Epoch 1087/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 27.8765 - val_loss: 0.2261\n",
      "Epoch 1088/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.0417 - val_loss: 0.3212\n",
      "Epoch 1089/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 10.9204 - val_loss: 0.5676\n",
      "Epoch 1090/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 11.7638 - val_loss: 0.7071\n",
      "Epoch 1091/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 5.5839 - val_loss: 0.7865\n",
      "Epoch 1092/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.4533 - val_loss: 0.8311\n",
      "Epoch 1093/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 13.0564 - val_loss: 0.8201\n",
      "Epoch 1094/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 18.3165 - val_loss: 0.7618\n",
      "Epoch 1095/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 10.4924 - val_loss: 0.7356\n",
      "Epoch 1096/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.4040 - val_loss: 0.6394\n",
      "Epoch 1097/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 18.9833 - val_loss: 0.5163\n",
      "Epoch 1098/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 5.3020 - val_loss: 0.3996\n",
      "Epoch 1099/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 11.5057 - val_loss: 0.3213\n",
      "Epoch 1100/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 6.3943 - val_loss: 0.3085\n",
      "Epoch 1101/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 6.5009 - val_loss: 0.3294\n",
      "Epoch 1102/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 8.1494 - val_loss: 0.4059\n",
      "Epoch 1103/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 11.4980 - val_loss: 0.5475\n",
      "Epoch 1104/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 8.0329 - val_loss: 0.8363\n",
      "Epoch 1105/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 5.7093 - val_loss: 1.0906\n",
      "Epoch 1106/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 11.0671 - val_loss: 1.2679\n",
      "Epoch 1107/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 11.4057 - val_loss: 1.2121\n",
      "Epoch 1108/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 6.6602 - val_loss: 1.0587\n",
      "Epoch 1109/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 4.5755 - val_loss: 0.9590\n",
      "Epoch 1110/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 12.1574 - val_loss: 0.8394\n",
      "Epoch 1111/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 7.7287 - val_loss: 0.6565\n",
      "Epoch 1112/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.7321 - val_loss: 0.4816\n",
      "Epoch 1113/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 8.3734 - val_loss: 0.3029\n",
      "Epoch 1114/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 7.5493 - val_loss: 0.2071\n",
      "Epoch 1115/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 6.9234 - val_loss: 0.1716\n",
      "Epoch 1116/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 6.5533 - val_loss: 0.1640\n",
      "Epoch 1117/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 10.4094 - val_loss: 0.1681\n",
      "Epoch 1118/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 5.5637 - val_loss: 0.2208\n",
      "Epoch 1119/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 23.4650 - val_loss: 0.3243\n",
      "Epoch 1120/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 6.1784 - val_loss: 0.5076\n",
      "Epoch 1121/2000\n",
      "81/81 [==============================] - 0s 113us/step - loss: 5.7542 - val_loss: 0.6887\n",
      "Epoch 1122/2000\n",
      "81/81 [==============================] - 0s 124us/step - loss: 5.9972 - val_loss: 0.8555\n",
      "Epoch 1123/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 6.7351 - val_loss: 1.0276\n",
      "Epoch 1124/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 5.0917 - val_loss: 1.1067\n",
      "Epoch 1125/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 8.5180 - val_loss: 1.0630\n",
      "Epoch 1126/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 6.0291 - val_loss: 0.8892\n",
      "Epoch 1127/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 8.5000 - val_loss: 0.6384\n",
      "Epoch 1128/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 28.7272 - val_loss: 0.3883\n",
      "Epoch 1129/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 6.0268 - val_loss: 0.2696\n",
      "Epoch 1130/2000\n",
      "81/81 [==============================] - 0s 120us/step - loss: 11.6819 - val_loss: 0.2164\n",
      "Epoch 1131/2000\n",
      "81/81 [==============================] - 0s 105us/step - loss: 4.6682 - val_loss: 0.2211\n",
      "Epoch 1132/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 4.3966 - val_loss: 0.2622\n",
      "Epoch 1133/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 3.9345 - val_loss: 0.3298\n",
      "Epoch 1134/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 7.9483 - val_loss: 0.4190\n",
      "Epoch 1135/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 5.8800 - val_loss: 0.5308\n",
      "Epoch 1136/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 5.8243 - val_loss: 0.5704\n",
      "Epoch 1137/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 7.1892 - val_loss: 0.6431\n",
      "Epoch 1138/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 12.3512 - val_loss: 0.7034\n",
      "Epoch 1139/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 4.9355 - val_loss: 0.6942\n",
      "Epoch 1140/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 6.5542 - val_loss: 0.7282\n",
      "Epoch 1141/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 7.7533 - val_loss: 0.6604\n",
      "Epoch 1142/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.3979 - val_loss: 0.5625\n",
      "Epoch 1143/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 9.0498 - val_loss: 0.5111\n",
      "Epoch 1144/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 7.6394 - val_loss: 0.5062\n",
      "Epoch 1145/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 7.2436 - val_loss: 0.5200\n",
      "Epoch 1146/2000\n",
      "81/81 [==============================] - 0s 49us/step - loss: 15.8098 - val_loss: 0.5859\n",
      "Epoch 1147/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 11.4362 - val_loss: 0.6001\n",
      "Epoch 1148/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 6.8115 - val_loss: 0.5975\n",
      "Epoch 1149/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 6.9108 - val_loss: 0.5008\n",
      "Epoch 1150/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 5.2628 - val_loss: 0.4155\n",
      "Epoch 1151/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 6.3335 - val_loss: 0.4726\n",
      "Epoch 1152/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 10.0609 - val_loss: 0.5641\n",
      "Epoch 1153/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 4.8580 - val_loss: 0.6662\n",
      "Epoch 1154/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 8.1832 - val_loss: 0.8476\n",
      "Epoch 1155/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 4.1946 - val_loss: 0.9958\n",
      "Epoch 1156/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 10.4188 - val_loss: 1.0228\n",
      "Epoch 1157/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 5.5551 - val_loss: 1.0512\n",
      "Epoch 1158/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 8.7886 - val_loss: 0.9394\n",
      "Epoch 1159/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 4.9725 - val_loss: 0.7915\n",
      "Epoch 1160/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 5.9582 - val_loss: 0.6924\n",
      "Epoch 1161/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 12.0216 - val_loss: 0.7062\n",
      "Epoch 1162/2000\n",
      "81/81 [==============================] - 0s 171us/step - loss: 6.9793 - val_loss: 0.7289\n",
      "Epoch 1163/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 8.8094 - val_loss: 0.6300\n",
      "Epoch 1164/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 5.5973 - val_loss: 0.6028\n",
      "Epoch 1165/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 6.8220 - val_loss: 0.6449\n",
      "Epoch 1166/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 3.5047 - val_loss: 0.6793\n",
      "Epoch 1167/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 4.4268 - val_loss: 0.7330\n",
      "Epoch 1168/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.8229 - val_loss: 0.8344\n",
      "Epoch 1169/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 4.5541 - val_loss: 0.7612\n",
      "Epoch 1170/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 12.3046 - val_loss: 0.6332\n",
      "Epoch 1171/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 3.4165 - val_loss: 0.5323\n",
      "Epoch 1172/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 4.3299 - val_loss: 0.4629\n",
      "Epoch 1173/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 8.0874 - val_loss: 0.4418\n",
      "Epoch 1174/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 8.1410 - val_loss: 0.4227\n",
      "Epoch 1175/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 5.1118 - val_loss: 0.3979\n",
      "Epoch 1176/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 4.5800 - val_loss: 0.3741\n",
      "Epoch 1177/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 4.2243 - val_loss: 0.3880\n",
      "Epoch 1178/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 7.0991 - val_loss: 0.5003\n",
      "Epoch 1179/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 9.3087 - val_loss: 0.6802\n",
      "Epoch 1180/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 5.2686 - val_loss: 0.8999\n",
      "Epoch 1181/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 8.3772 - val_loss: 1.1070\n",
      "Epoch 1182/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 5.0672 - val_loss: 1.0660\n",
      "Epoch 1183/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 3.3875 - val_loss: 0.8430\n",
      "Epoch 1184/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 7.2294 - val_loss: 0.6352\n",
      "Epoch 1185/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 8.1454 - val_loss: 0.4280\n",
      "Epoch 1186/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 7.2294 - val_loss: 0.2846\n",
      "Epoch 1187/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 4.3960 - val_loss: 0.2604\n",
      "Epoch 1188/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 11.0853 - val_loss: 0.2671\n",
      "Epoch 1189/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 4.8890 - val_loss: 0.2895\n",
      "Epoch 1190/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 7.7544 - val_loss: 0.3281\n",
      "Epoch 1191/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.2840 - val_loss: 0.3685\n",
      "Epoch 1192/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 4.4511 - val_loss: 0.4245\n",
      "Epoch 1193/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 12.3937 - val_loss: 0.4197\n",
      "Epoch 1194/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 5.1067 - val_loss: 0.4120\n",
      "Epoch 1195/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.6203 - val_loss: 0.3941\n",
      "Epoch 1196/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 5.7028 - val_loss: 0.4007\n",
      "Epoch 1197/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 4.6330 - val_loss: 0.4092\n",
      "Epoch 1198/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 4.8620 - val_loss: 0.4135\n",
      "Epoch 1199/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 8.2284 - val_loss: 0.4090\n",
      "Epoch 1200/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 6.5814 - val_loss: 0.3974\n",
      "Epoch 1201/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 5.3127 - val_loss: 0.4094\n",
      "Epoch 1202/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 11.6235 - val_loss: 0.3511\n",
      "Epoch 1203/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 4.4189 - val_loss: 0.3347\n",
      "Epoch 1204/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 6.6724 - val_loss: 0.3664\n",
      "Epoch 1205/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 10.0704 - val_loss: 0.4570\n",
      "Epoch 1206/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 5.8448 - val_loss: 0.5827\n",
      "Epoch 1207/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 5.9346 - val_loss: 0.6265\n",
      "Epoch 1208/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 4.2046 - val_loss: 0.6511\n",
      "Epoch 1209/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 7.2277 - val_loss: 0.6830\n",
      "Epoch 1210/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 7.9749 - val_loss: 0.6809\n",
      "Epoch 1211/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 14.2593 - val_loss: 0.6188\n",
      "Epoch 1212/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 5.9583 - val_loss: 0.6217\n",
      "Epoch 1213/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 4.9416 - val_loss: 0.6114\n",
      "Epoch 1214/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 8.8564 - val_loss: 0.5604\n",
      "Epoch 1215/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 9.8726 - val_loss: 0.5344\n",
      "Epoch 1216/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.9530 - val_loss: 0.4719\n",
      "Epoch 1217/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 6.7778 - val_loss: 0.4461\n",
      "Epoch 1218/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 4.0251 - val_loss: 0.4492\n",
      "Epoch 1219/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 7.1685 - val_loss: 0.5563\n",
      "Epoch 1220/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 10.5039 - val_loss: 0.5399\n",
      "Epoch 1221/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 4.1239 - val_loss: 0.5026\n",
      "Epoch 1222/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 5.7839 - val_loss: 0.5313\n",
      "Epoch 1223/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 6.7442 - val_loss: 0.6139\n",
      "Epoch 1224/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 7.5484 - val_loss: 0.6920\n",
      "Epoch 1225/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 4.8301 - val_loss: 0.7579\n",
      "Epoch 1226/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 3.5273 - val_loss: 0.8328\n",
      "Epoch 1227/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 9.4738 - val_loss: 0.8910\n",
      "Epoch 1228/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 3.6986 - val_loss: 0.8438\n",
      "Epoch 1229/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 5.1557 - val_loss: 0.7663\n",
      "Epoch 1230/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 9.8492 - val_loss: 0.7108\n",
      "Epoch 1231/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 4.4035 - val_loss: 0.6876\n",
      "Epoch 1232/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 6.9762 - val_loss: 0.6501\n",
      "Epoch 1233/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 3.4328 - val_loss: 0.6188\n",
      "Epoch 1234/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 8.6034 - val_loss: 0.5665\n",
      "Epoch 1235/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.7485 - val_loss: 0.4891\n",
      "Epoch 1236/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 6.6815 - val_loss: 0.4758\n",
      "Epoch 1237/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 11.5068 - val_loss: 0.4450\n",
      "Epoch 1238/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 5.4160 - val_loss: 0.4627\n",
      "Epoch 1239/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 5.6847 - val_loss: 0.4981\n",
      "Epoch 1240/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 6.4829 - val_loss: 0.5997\n",
      "Epoch 1241/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 6.9763 - val_loss: 0.6756\n",
      "Epoch 1242/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.4232 - val_loss: 0.6846\n",
      "Epoch 1243/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 7.1221 - val_loss: 0.8180\n",
      "Epoch 1244/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 8.1881 - val_loss: 0.8855\n",
      "Epoch 1245/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 5.8997 - val_loss: 1.0092\n",
      "Epoch 1246/2000\n",
      "81/81 [==============================] - 0s 361us/step - loss: 10.3588 - val_loss: 0.9872\n",
      "Epoch 1247/2000\n",
      "81/81 [==============================] - 0s 105us/step - loss: 3.8063 - val_loss: 0.9274\n",
      "Epoch 1248/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 7.8486 - val_loss: 0.8067\n",
      "Epoch 1249/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 6.3950 - val_loss: 0.6332\n",
      "Epoch 1250/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 5.7093 - val_loss: 0.5743\n",
      "Epoch 1251/2000\n",
      "81/81 [==============================] - 0s 50us/step - loss: 4.2813 - val_loss: 0.5741\n",
      "Epoch 1252/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 7.6407 - val_loss: 0.5651\n",
      "Epoch 1253/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.5068 - val_loss: 0.5493\n",
      "Epoch 1254/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 11.3852 - val_loss: 0.5052\n",
      "Epoch 1255/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 6.1906 - val_loss: 0.5290\n",
      "Epoch 1256/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 5.2604 - val_loss: 0.5856\n",
      "Epoch 1257/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 6.7246 - val_loss: 0.6537\n",
      "Epoch 1258/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 7.9498 - val_loss: 0.8327\n",
      "Epoch 1259/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 5.3808 - val_loss: 0.9692\n",
      "Epoch 1260/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 3.1697 - val_loss: 1.0797\n",
      "Epoch 1261/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 6.4882 - val_loss: 1.0569\n",
      "Epoch 1262/2000\n",
      "81/81 [==============================] - 0s 50us/step - loss: 6.6493 - val_loss: 0.8049\n",
      "Epoch 1263/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 9.7725 - val_loss: 0.5484\n",
      "Epoch 1264/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 10.2433 - val_loss: 0.4104\n",
      "Epoch 1265/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 4.9049 - val_loss: 0.3229\n",
      "Epoch 1266/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.9366 - val_loss: 0.2813\n",
      "Epoch 1267/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 7.1703 - val_loss: 0.3182\n",
      "Epoch 1268/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 4.6764 - val_loss: 0.3617\n",
      "Epoch 1269/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 4.2742 - val_loss: 0.4495\n",
      "Epoch 1270/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 4.4570 - val_loss: 0.6056\n",
      "Epoch 1271/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 4.7964 - val_loss: 0.8728\n",
      "Epoch 1272/2000\n",
      "81/81 [==============================] - 0s 47us/step - loss: 4.5294 - val_loss: 1.1031\n",
      "Epoch 1273/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 5.7906 - val_loss: 1.3247\n",
      "Epoch 1274/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 3.2632 - val_loss: 1.3485\n",
      "Epoch 1275/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 4.5197 - val_loss: 1.2889\n",
      "Epoch 1276/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 5.5557 - val_loss: 1.0376\n",
      "Epoch 1277/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 3.2003 - val_loss: 0.7196\n",
      "Epoch 1278/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 6.4637 - val_loss: 0.5681\n",
      "Epoch 1279/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 5.8079 - val_loss: 0.4683\n",
      "Epoch 1280/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 4.4277 - val_loss: 0.4600\n",
      "Epoch 1281/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 5.2893 - val_loss: 0.4881\n",
      "Epoch 1282/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 5.2445 - val_loss: 0.5406\n",
      "Epoch 1283/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 5.6434 - val_loss: 0.6210\n",
      "Epoch 1284/2000\n",
      "81/81 [==============================] - 0s 159us/step - loss: 3.8354 - val_loss: 0.6370\n",
      "Epoch 1285/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 8.3345 - val_loss: 0.5552\n",
      "Epoch 1286/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 4.3825 - val_loss: 0.5009\n",
      "Epoch 1287/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 6.6180 - val_loss: 0.4735\n",
      "Epoch 1288/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 4.6939 - val_loss: 0.4535\n",
      "Epoch 1289/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 7.2916 - val_loss: 0.4333\n",
      "Epoch 1290/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 4.3733 - val_loss: 0.5096\n",
      "Epoch 1291/2000\n",
      "81/81 [==============================] - 0s 163us/step - loss: 3.6813 - val_loss: 0.6275\n",
      "Epoch 1292/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 4.3752 - val_loss: 0.7305\n",
      "Epoch 1293/2000\n",
      "81/81 [==============================] - 0s 118us/step - loss: 5.9884 - val_loss: 0.6935\n",
      "Epoch 1294/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 7.4428 - val_loss: 0.6053\n",
      "Epoch 1295/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 6.1895 - val_loss: 0.5361\n",
      "Epoch 1296/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 4.0780 - val_loss: 0.4852\n",
      "Epoch 1297/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.9047 - val_loss: 0.4563\n",
      "Epoch 1298/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 11.6592 - val_loss: 0.4073\n",
      "Epoch 1299/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 3.2421 - val_loss: 0.3648\n",
      "Epoch 1300/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 4.6492 - val_loss: 0.3576\n",
      "Epoch 1301/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 4.4227 - val_loss: 0.3774\n",
      "Epoch 1302/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 12.2200 - val_loss: 0.4887\n",
      "Epoch 1303/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 6.5051 - val_loss: 0.5504\n",
      "Epoch 1304/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 4.2029 - val_loss: 0.6314\n",
      "Epoch 1305/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 5.5768 - val_loss: 0.7674\n",
      "Epoch 1306/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 3.7714 - val_loss: 0.8490\n",
      "Epoch 1307/2000\n",
      "81/81 [==============================] - 0s 109us/step - loss: 3.2311 - val_loss: 0.8843\n",
      "Epoch 1308/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 11.2733 - val_loss: 0.8530\n",
      "Epoch 1309/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 4.6285 - val_loss: 0.8048\n",
      "Epoch 1310/2000\n",
      "81/81 [==============================] - 0s 112us/step - loss: 5.0136 - val_loss: 0.6526\n",
      "Epoch 1311/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.4687 - val_loss: 0.4613\n",
      "Epoch 1312/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.3509 - val_loss: 0.3335\n",
      "Epoch 1313/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 5.8375 - val_loss: 0.2765\n",
      "Epoch 1314/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 5.7315 - val_loss: 0.2488\n",
      "Epoch 1315/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 4.0355 - val_loss: 0.2610\n",
      "Epoch 1316/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 10.3152 - val_loss: 0.2856\n",
      "Epoch 1317/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 7.0813 - val_loss: 0.3369\n",
      "Epoch 1318/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 9.9509 - val_loss: 0.4639\n",
      "Epoch 1319/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 6.5143 - val_loss: 0.5513\n",
      "Epoch 1320/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 4.6323 - val_loss: 0.6349\n",
      "Epoch 1321/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 8.4228 - val_loss: 0.8398\n",
      "Epoch 1322/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 5.8728 - val_loss: 0.9750\n",
      "Epoch 1323/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 8.1658 - val_loss: 0.9769\n",
      "Epoch 1324/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 5.7648 - val_loss: 0.9097\n",
      "Epoch 1325/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 6.8465 - val_loss: 0.8780\n",
      "Epoch 1326/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 7.9234 - val_loss: 0.6703\n",
      "Epoch 1327/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 5.9134 - val_loss: 0.4708\n",
      "Epoch 1328/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 4.5639 - val_loss: 0.4141\n",
      "Epoch 1329/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 5.4697 - val_loss: 0.3374\n",
      "Epoch 1330/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 8.5045 - val_loss: 0.2799\n",
      "Epoch 1331/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 5.1284 - val_loss: 0.2536\n",
      "Epoch 1332/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 7.7052 - val_loss: 0.2357\n",
      "Epoch 1333/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 5.6174 - val_loss: 0.2562\n",
      "Epoch 1334/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 8.6861 - val_loss: 0.2790\n",
      "Epoch 1335/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 5.7624 - val_loss: 0.3650\n",
      "Epoch 1336/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 7.4773 - val_loss: 0.4120\n",
      "Epoch 1337/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 4.6525 - val_loss: 0.4389\n",
      "Epoch 1338/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 4.4776 - val_loss: 0.4895\n",
      "Epoch 1339/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 4.1925 - val_loss: 0.5292\n",
      "Epoch 1340/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 8.2551 - val_loss: 0.5490\n",
      "Epoch 1341/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 5.3329 - val_loss: 0.5742\n",
      "Epoch 1342/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 3.7406 - val_loss: 0.5295\n",
      "Epoch 1343/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 3.3701 - val_loss: 0.4487\n",
      "Epoch 1344/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 5.0096 - val_loss: 0.3910\n",
      "Epoch 1345/2000\n",
      "81/81 [==============================] - 0s 113us/step - loss: 3.7026 - val_loss: 0.4209\n",
      "Epoch 1346/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 3.8242 - val_loss: 0.4487\n",
      "Epoch 1347/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 4.5442 - val_loss: 0.4407\n",
      "Epoch 1348/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 5.5941 - val_loss: 0.4571\n",
      "Epoch 1349/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 5.7746 - val_loss: 0.4267\n",
      "Epoch 1350/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 4.2479 - val_loss: 0.4245\n",
      "Epoch 1351/2000\n",
      "81/81 [==============================] - 0s 110us/step - loss: 10.4374 - val_loss: 0.4374\n",
      "Epoch 1352/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 7.4388 - val_loss: 0.4397\n",
      "Epoch 1353/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 5.3849 - val_loss: 0.4789\n",
      "Epoch 1354/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 5.6812 - val_loss: 0.5845\n",
      "Epoch 1355/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 5.9073 - val_loss: 0.7261\n",
      "Epoch 1356/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 3.3596 - val_loss: 0.8782\n",
      "Epoch 1357/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 6.3375 - val_loss: 0.9780\n",
      "Epoch 1358/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 5.1165 - val_loss: 1.0616\n",
      "Epoch 1359/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.8441 - val_loss: 0.9787\n",
      "Epoch 1360/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 3.7685 - val_loss: 0.8107\n",
      "Epoch 1361/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 4.8637 - val_loss: 0.6433\n",
      "Epoch 1362/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 5.2034 - val_loss: 0.4760\n",
      "Epoch 1363/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 6.4535 - val_loss: 0.3989\n",
      "Epoch 1364/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 3.2951 - val_loss: 0.3587\n",
      "Epoch 1365/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 4.4624 - val_loss: 0.3807\n",
      "Epoch 1366/2000\n",
      "81/81 [==============================] - 0s 113us/step - loss: 4.3468 - val_loss: 0.3776\n",
      "Epoch 1367/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 10.3076 - val_loss: 0.4021\n",
      "Epoch 1368/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 4.9613 - val_loss: 0.5039\n",
      "Epoch 1369/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 4.6373 - val_loss: 0.6709\n",
      "Epoch 1370/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 7.5460 - val_loss: 0.8233\n",
      "Epoch 1371/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 4.1134 - val_loss: 0.9566\n",
      "Epoch 1372/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 3.7306 - val_loss: 1.0056\n",
      "Epoch 1373/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 10.1435 - val_loss: 0.8441\n",
      "Epoch 1374/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 7.4649 - val_loss: 0.6582\n",
      "Epoch 1375/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 4.5447 - val_loss: 0.4767\n",
      "Epoch 1376/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 3.0148 - val_loss: 0.3908\n",
      "Epoch 1377/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 3.4789 - val_loss: 0.3423\n",
      "Epoch 1378/2000\n",
      "81/81 [==============================] - 0s 145us/step - loss: 3.4965 - val_loss: 0.2655\n",
      "Epoch 1379/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 6.2821 - val_loss: 0.2280\n",
      "Epoch 1380/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 4.5441 - val_loss: 0.2399\n",
      "Epoch 1381/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 4.5305 - val_loss: 0.2891\n",
      "Epoch 1382/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 5.4003 - val_loss: 0.3680\n",
      "Epoch 1383/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 5.0733 - val_loss: 0.4638\n",
      "Epoch 1384/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 10.0470 - val_loss: 0.6294\n",
      "Epoch 1385/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 7.5246 - val_loss: 0.8640\n",
      "Epoch 1386/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 5.3838 - val_loss: 0.9721\n",
      "Epoch 1387/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.8921 - val_loss: 1.0300\n",
      "Epoch 1388/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 8.7634 - val_loss: 1.0128\n",
      "Epoch 1389/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 5.6028 - val_loss: 0.9095\n",
      "Epoch 1390/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 3.9710 - val_loss: 0.7912\n",
      "Epoch 1391/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.1947 - val_loss: 0.7036\n",
      "Epoch 1392/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 4.0437 - val_loss: 0.5815\n",
      "Epoch 1393/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 6.6163 - val_loss: 0.4448\n",
      "Epoch 1394/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 4.3862 - val_loss: 0.3929\n",
      "Epoch 1395/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 5.3336 - val_loss: 0.4082\n",
      "Epoch 1396/2000\n",
      "81/81 [==============================] - 0s 112us/step - loss: 4.9420 - val_loss: 0.4452\n",
      "Epoch 1397/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 4.5545 - val_loss: 0.5835\n",
      "Epoch 1398/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 4.4627 - val_loss: 0.6680\n",
      "Epoch 1399/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 3.8774 - val_loss: 0.7903\n",
      "Epoch 1400/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 7.1625 - val_loss: 0.7744\n",
      "Epoch 1401/2000\n",
      "81/81 [==============================] - 0s 112us/step - loss: 8.2650 - val_loss: 0.6470\n",
      "Epoch 1402/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 4.7118 - val_loss: 0.5402\n",
      "Epoch 1403/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.2648 - val_loss: 0.4593\n",
      "Epoch 1404/2000\n",
      "81/81 [==============================] - 0s 124us/step - loss: 3.0623 - val_loss: 0.4123\n",
      "Epoch 1405/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 5.4039 - val_loss: 0.4707\n",
      "Epoch 1406/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 2.6787 - val_loss: 0.5264\n",
      "Epoch 1407/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 9.0343 - val_loss: 0.6059\n",
      "Epoch 1408/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 5.7411 - val_loss: 0.7180\n",
      "Epoch 1409/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 3.2153 - val_loss: 0.7431\n",
      "Epoch 1410/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 6.1170 - val_loss: 0.7531\n",
      "Epoch 1411/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 3.5755 - val_loss: 0.7591\n",
      "Epoch 1412/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 4.5511 - val_loss: 0.6715\n",
      "Epoch 1413/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 5.6398 - val_loss: 0.6078\n",
      "Epoch 1414/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 4.7701 - val_loss: 0.5119\n",
      "Epoch 1415/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 3.0674 - val_loss: 0.4392\n",
      "Epoch 1416/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 3.5411 - val_loss: 0.3999\n",
      "Epoch 1417/2000\n",
      "81/81 [==============================] - 0s 113us/step - loss: 4.7973 - val_loss: 0.4268\n",
      "Epoch 1418/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 6.7402 - val_loss: 0.5117\n",
      "Epoch 1419/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 4.3803 - val_loss: 0.6379\n",
      "Epoch 1420/2000\n",
      "81/81 [==============================] - 0s 116us/step - loss: 2.3359 - val_loss: 0.7425\n",
      "Epoch 1421/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 7.1898 - val_loss: 0.7637\n",
      "Epoch 1422/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 4.8754 - val_loss: 0.7486\n",
      "Epoch 1423/2000\n",
      "81/81 [==============================] - 0s 131us/step - loss: 8.3845 - val_loss: 0.7079\n",
      "Epoch 1424/2000\n",
      "81/81 [==============================] - 0s 121us/step - loss: 3.2110 - val_loss: 0.7216\n",
      "Epoch 1425/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 2.9747 - val_loss: 0.7953\n",
      "Epoch 1426/2000\n",
      "81/81 [==============================] - 0s 109us/step - loss: 6.1833 - val_loss: 0.8970\n",
      "Epoch 1427/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 5.1119 - val_loss: 0.9738\n",
      "Epoch 1428/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 6.9372 - val_loss: 0.9571\n",
      "Epoch 1429/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 11.7554 - val_loss: 0.7916\n",
      "Epoch 1430/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 5.8783 - val_loss: 0.6889\n",
      "Epoch 1431/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.3302 - val_loss: 0.5737\n",
      "Epoch 1432/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 5.5037 - val_loss: 0.5876\n",
      "Epoch 1433/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 4.1318 - val_loss: 0.5901\n",
      "Epoch 1434/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 5.0172 - val_loss: 0.5826\n",
      "Epoch 1435/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 3.3337 - val_loss: 0.5878\n",
      "Epoch 1436/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 5.4801 - val_loss: 0.5746\n",
      "Epoch 1437/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 3.2887 - val_loss: 0.5288\n",
      "Epoch 1438/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 4.6990 - val_loss: 0.4718\n",
      "Epoch 1439/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 7.6006 - val_loss: 0.4731\n",
      "Epoch 1440/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 4.8389 - val_loss: 0.4755\n",
      "Epoch 1441/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 3.7657 - val_loss: 0.5677\n",
      "Epoch 1442/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 5.1113 - val_loss: 0.7821\n",
      "Epoch 1443/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 6.6614 - val_loss: 0.9306\n",
      "Epoch 1444/2000\n",
      "81/81 [==============================] - 0s 95us/step - loss: 5.2209 - val_loss: 0.9443\n",
      "Epoch 1445/2000\n",
      "81/81 [==============================] - 0s 114us/step - loss: 3.6296 - val_loss: 0.8719\n",
      "Epoch 1446/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 6.6309 - val_loss: 0.7123\n",
      "Epoch 1447/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 3.6147 - val_loss: 0.5677\n",
      "Epoch 1448/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 4.0002 - val_loss: 0.5175\n",
      "Epoch 1449/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 6.2203 - val_loss: 0.6150\n",
      "Epoch 1450/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 4.4277 - val_loss: 0.7652\n",
      "Epoch 1451/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 3.4014 - val_loss: 0.9817\n",
      "Epoch 1452/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 4.3572 - val_loss: 1.0897\n",
      "Epoch 1453/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 4.4701 - val_loss: 1.0977\n",
      "Epoch 1454/2000\n",
      "81/81 [==============================] - 0s 98us/step - loss: 4.1837 - val_loss: 1.0731\n",
      "Epoch 1455/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 8.1779 - val_loss: 0.8928\n",
      "Epoch 1456/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 5.4681 - val_loss: 0.6671\n",
      "Epoch 1457/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 8.2456 - val_loss: 0.4286\n",
      "Epoch 1458/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 2.4417 - val_loss: 0.2920\n",
      "Epoch 1459/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 2.5411 - val_loss: 0.2381\n",
      "Epoch 1460/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 5.9913 - val_loss: 0.2323\n",
      "Epoch 1461/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 2.2164 - val_loss: 0.2641\n",
      "Epoch 1462/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 3.4288 - val_loss: 0.3982\n",
      "Epoch 1463/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 2.8664 - val_loss: 0.5768\n",
      "Epoch 1464/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 4.1042 - val_loss: 0.8379\n",
      "Epoch 1465/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 5.4965 - val_loss: 1.2068\n",
      "Epoch 1466/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 5.7982 - val_loss: 1.2853\n",
      "Epoch 1467/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 2.7138 - val_loss: 1.2105\n",
      "Epoch 1468/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 4.9551 - val_loss: 1.0857\n",
      "Epoch 1469/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 2.7291 - val_loss: 1.0106\n",
      "Epoch 1470/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 3.1577 - val_loss: 0.8676\n",
      "Epoch 1471/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 5.4712 - val_loss: 0.7322\n",
      "Epoch 1472/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 4.4539 - val_loss: 0.5321\n",
      "Epoch 1473/2000\n",
      "81/81 [==============================] - 0s 107us/step - loss: 3.4513 - val_loss: 0.3724\n",
      "Epoch 1474/2000\n",
      "81/81 [==============================] - 0s 108us/step - loss: 4.5658 - val_loss: 0.2869\n",
      "Epoch 1475/2000\n",
      "81/81 [==============================] - 0s 98us/step - loss: 5.9329 - val_loss: 0.2788\n",
      "Epoch 1476/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 3.6030 - val_loss: 0.3074\n",
      "Epoch 1477/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 5.8375 - val_loss: 0.3701\n",
      "Epoch 1478/2000\n",
      "81/81 [==============================] - 0s 98us/step - loss: 3.0402 - val_loss: 0.4985\n",
      "Epoch 1479/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 6.2176 - val_loss: 0.7323\n",
      "Epoch 1480/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 5.2691 - val_loss: 1.0714\n",
      "Epoch 1481/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 5.2515 - val_loss: 1.2648\n",
      "Epoch 1482/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 3.2271 - val_loss: 1.3740\n",
      "Epoch 1483/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.2638 - val_loss: 1.2837\n",
      "Epoch 1484/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 4.0186 - val_loss: 1.0157\n",
      "Epoch 1485/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 3.5557 - val_loss: 0.6279\n",
      "Epoch 1486/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 2.5141 - val_loss: 0.3672\n",
      "Epoch 1487/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 5.9450 - val_loss: 0.2446\n",
      "Epoch 1488/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 4.4545 - val_loss: 0.2379\n",
      "Epoch 1489/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 4.1561 - val_loss: 0.3354\n",
      "Epoch 1490/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 2.8416 - val_loss: 0.4787\n",
      "Epoch 1491/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 4.4205 - val_loss: 0.7516\n",
      "Epoch 1492/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 2.8016 - val_loss: 1.0579\n",
      "Epoch 1493/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 3.4544 - val_loss: 1.1769\n",
      "Epoch 1494/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 2.5070 - val_loss: 1.3129\n",
      "Epoch 1495/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 3.9917 - val_loss: 1.2529\n",
      "Epoch 1496/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 6.9707 - val_loss: 0.9944\n",
      "Epoch 1497/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 7.9232 - val_loss: 0.6886\n",
      "Epoch 1498/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 3.8570 - val_loss: 0.4455\n",
      "Epoch 1499/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 8.3809 - val_loss: 0.3117\n",
      "Epoch 1500/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 3.2587 - val_loss: 0.2727\n",
      "Epoch 1501/2000\n",
      "81/81 [==============================] - 0s 137us/step - loss: 3.0616 - val_loss: 0.2485\n",
      "Epoch 1502/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 2.8150 - val_loss: 0.3399\n",
      "Epoch 1503/2000\n",
      "81/81 [==============================] - 0s 95us/step - loss: 2.7689 - val_loss: 0.4784\n",
      "Epoch 1504/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 3.7590 - val_loss: 0.6442\n",
      "Epoch 1505/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 4.2005 - val_loss: 0.8934\n",
      "Epoch 1506/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 4.4097 - val_loss: 1.0989\n",
      "Epoch 1507/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 4.1805 - val_loss: 1.1473\n",
      "Epoch 1508/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 4.0713 - val_loss: 1.0259\n",
      "Epoch 1509/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 3.4009 - val_loss: 0.7899\n",
      "Epoch 1510/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 3.6463 - val_loss: 0.6757\n",
      "Epoch 1511/2000\n",
      "81/81 [==============================] - 0s 98us/step - loss: 6.6683 - val_loss: 0.5209\n",
      "Epoch 1512/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 5.4232 - val_loss: 0.4640\n",
      "Epoch 1513/2000\n",
      "81/81 [==============================] - 0s 113us/step - loss: 3.3639 - val_loss: 0.5337\n",
      "Epoch 1514/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 4.0660 - val_loss: 0.6936\n",
      "Epoch 1515/2000\n",
      "81/81 [==============================] - 0s 117us/step - loss: 4.4267 - val_loss: 0.9418\n",
      "Epoch 1516/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 4.1601 - val_loss: 1.3023\n",
      "Epoch 1517/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 7.0925 - val_loss: 1.4084\n",
      "Epoch 1518/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 3.4079 - val_loss: 1.3537\n",
      "Epoch 1519/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 3.1001 - val_loss: 1.1523\n",
      "Epoch 1520/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 5.3936 - val_loss: 0.8594\n",
      "Epoch 1521/2000\n",
      "81/81 [==============================] - 0s 110us/step - loss: 4.9558 - val_loss: 0.6557\n",
      "Epoch 1522/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 5.5981 - val_loss: 0.5879\n",
      "Epoch 1523/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 3.3613 - val_loss: 0.5660\n",
      "Epoch 1524/2000\n",
      "81/81 [==============================] - 0s 95us/step - loss: 3.6777 - val_loss: 0.5839\n",
      "Epoch 1525/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 3.9796 - val_loss: 0.6754\n",
      "Epoch 1526/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 5.4817 - val_loss: 0.7726\n",
      "Epoch 1527/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 2.9812 - val_loss: 0.8416\n",
      "Epoch 1528/2000\n",
      "81/81 [==============================] - 0s 99us/step - loss: 9.2971 - val_loss: 0.8762\n",
      "Epoch 1529/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 3.8818 - val_loss: 0.9340\n",
      "Epoch 1530/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 8.8207 - val_loss: 0.9479\n",
      "Epoch 1531/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 3.8308 - val_loss: 0.8398\n",
      "Epoch 1532/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 1.8187 - val_loss: 0.7419\n",
      "Epoch 1533/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 2.3673 - val_loss: 0.6165\n",
      "Epoch 1534/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 2.9204 - val_loss: 0.4817\n",
      "Epoch 1535/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 8.4918 - val_loss: 0.4739\n",
      "Epoch 1536/2000\n",
      "81/81 [==============================] - 0s 98us/step - loss: 9.4810 - val_loss: 0.5300\n",
      "Epoch 1537/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 3.5893 - val_loss: 0.5547\n",
      "Epoch 1538/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 3.7467 - val_loss: 0.6472\n",
      "Epoch 1539/2000\n",
      "81/81 [==============================] - 0s 123us/step - loss: 4.4660 - val_loss: 0.8221\n",
      "Epoch 1540/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 3.1142 - val_loss: 1.0188\n",
      "Epoch 1541/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 3.6995 - val_loss: 1.2354\n",
      "Epoch 1542/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 4.4749 - val_loss: 1.3117\n",
      "Epoch 1543/2000\n",
      "81/81 [==============================] - 0s 108us/step - loss: 1.5618 - val_loss: 1.1963\n",
      "Epoch 1544/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 3.7067 - val_loss: 0.9662\n",
      "Epoch 1545/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 2.9191 - val_loss: 0.6827\n",
      "Epoch 1546/2000\n",
      "81/81 [==============================] - 0s 122us/step - loss: 4.6850 - val_loss: 0.5355\n",
      "Epoch 1547/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 4.2259 - val_loss: 0.3923\n",
      "Epoch 1548/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 3.2568 - val_loss: 0.4029\n",
      "Epoch 1549/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 2.7773 - val_loss: 0.4410\n",
      "Epoch 1550/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 3.7410 - val_loss: 0.4327\n",
      "Epoch 1551/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.0527 - val_loss: 0.4433\n",
      "Epoch 1552/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 4.5550 - val_loss: 0.4854\n",
      "Epoch 1553/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 2.0954 - val_loss: 0.5137\n",
      "Epoch 1554/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 3.3491 - val_loss: 0.6238\n",
      "Epoch 1555/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 2.7196 - val_loss: 0.7901\n",
      "Epoch 1556/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 3.7039 - val_loss: 0.8745\n",
      "Epoch 1557/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 1.6837 - val_loss: 0.8895\n",
      "Epoch 1558/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 2.9460 - val_loss: 0.8603\n",
      "Epoch 1559/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 5.4325 - val_loss: 0.7607\n",
      "Epoch 1560/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 3.6202 - val_loss: 0.5664\n",
      "Epoch 1561/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 3.9561 - val_loss: 0.4633\n",
      "Epoch 1562/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 9.4093 - val_loss: 0.4315\n",
      "Epoch 1563/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 3.9860 - val_loss: 0.5191\n",
      "Epoch 1564/2000\n",
      "81/81 [==============================] - 0s 110us/step - loss: 3.6665 - val_loss: 0.6838\n",
      "Epoch 1565/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 5.6709 - val_loss: 0.8601\n",
      "Epoch 1566/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 4.1319 - val_loss: 0.9497\n",
      "Epoch 1567/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 5.6774 - val_loss: 0.9883\n",
      "Epoch 1568/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 6.5186 - val_loss: 1.0025\n",
      "Epoch 1569/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 4.6702 - val_loss: 0.8477\n",
      "Epoch 1570/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 4.4377 - val_loss: 0.6952\n",
      "Epoch 1571/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 4.2989 - val_loss: 0.5781\n",
      "Epoch 1572/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.9659 - val_loss: 0.5243\n",
      "Epoch 1573/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 3.2043 - val_loss: 0.4905\n",
      "Epoch 1574/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 3.4125 - val_loss: 0.4814\n",
      "Epoch 1575/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 3.2607 - val_loss: 0.4514\n",
      "Epoch 1576/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 5.8614 - val_loss: 0.5036\n",
      "Epoch 1577/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 8.7970 - val_loss: 0.5911\n",
      "Epoch 1578/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 3.7487 - val_loss: 0.7395\n",
      "Epoch 1579/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 2.6811 - val_loss: 0.9404\n",
      "Epoch 1580/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 3.5223 - val_loss: 1.2079\n",
      "Epoch 1581/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 8.0654 - val_loss: 1.1850\n",
      "Epoch 1582/2000\n",
      "81/81 [==============================] - 0s 109us/step - loss: 2.6396 - val_loss: 1.1046\n",
      "Epoch 1583/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 6.0868 - val_loss: 0.9471\n",
      "Epoch 1584/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 3.5942 - val_loss: 0.6912\n",
      "Epoch 1585/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 2.4074 - val_loss: 0.5646\n",
      "Epoch 1586/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 3.7259 - val_loss: 0.6069\n",
      "Epoch 1587/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 1.7466 - val_loss: 0.7338\n",
      "Epoch 1588/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 2.9124 - val_loss: 0.9149\n",
      "Epoch 1589/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 2.4320 - val_loss: 1.0204\n",
      "Epoch 1590/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 3.4442 - val_loss: 1.0303\n",
      "Epoch 1591/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 1.9623 - val_loss: 0.8926\n",
      "Epoch 1592/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 7.3395 - val_loss: 0.7473\n",
      "Epoch 1593/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 3.9360 - val_loss: 0.6442\n",
      "Epoch 1594/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 2.9119 - val_loss: 0.6495\n",
      "Epoch 1595/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 2.7637 - val_loss: 0.6100\n",
      "Epoch 1596/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 1.8428 - val_loss: 0.6143\n",
      "Epoch 1597/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 4.6219 - val_loss: 0.6226\n",
      "Epoch 1598/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 2.5849 - val_loss: 0.7376\n",
      "Epoch 1599/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 3.9354 - val_loss: 0.8861\n",
      "Epoch 1600/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 2.4213 - val_loss: 1.0176\n",
      "Epoch 1601/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 3.7193 - val_loss: 1.0735\n",
      "Epoch 1602/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 8.9744 - val_loss: 1.0280\n",
      "Epoch 1603/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 2.8886 - val_loss: 0.8137\n",
      "Epoch 1604/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.7002 - val_loss: 0.7086\n",
      "Epoch 1605/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.3376 - val_loss: 0.6282\n",
      "Epoch 1606/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 3.4071 - val_loss: 0.5579\n",
      "Epoch 1607/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.3705 - val_loss: 0.5323\n",
      "Epoch 1608/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 4.6817 - val_loss: 0.7135\n",
      "Epoch 1609/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 4.0317 - val_loss: 0.9569\n",
      "Epoch 1610/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.2334 - val_loss: 1.0624\n",
      "Epoch 1611/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 2.4638 - val_loss: 1.1627\n",
      "Epoch 1612/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 2.4927 - val_loss: 1.0324\n",
      "Epoch 1613/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 4.4901 - val_loss: 0.8411\n",
      "Epoch 1614/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 1.8907 - val_loss: 0.7126\n",
      "Epoch 1615/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 3.3216 - val_loss: 0.5628\n",
      "Epoch 1616/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 2.7831 - val_loss: 0.4317\n",
      "Epoch 1617/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 3.6813 - val_loss: 0.4073\n",
      "Epoch 1618/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 3.9362 - val_loss: 0.4509\n",
      "Epoch 1619/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 3.8806 - val_loss: 0.6154\n",
      "Epoch 1620/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 3.8820 - val_loss: 0.8920\n",
      "Epoch 1621/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 4.2353 - val_loss: 1.0805\n",
      "Epoch 1622/2000\n",
      "81/81 [==============================] - 0s 103us/step - loss: 4.2030 - val_loss: 1.1148\n",
      "Epoch 1623/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 2.1714 - val_loss: 1.1159\n",
      "Epoch 1624/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 3.9455 - val_loss: 1.0612\n",
      "Epoch 1625/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 3.4171 - val_loss: 0.9536\n",
      "Epoch 1626/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 4.3145 - val_loss: 0.7727\n",
      "Epoch 1627/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 2.7328 - val_loss: 0.5308\n",
      "Epoch 1628/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 4.6354 - val_loss: 0.4395\n",
      "Epoch 1629/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 1.8075 - val_loss: 0.4504\n",
      "Epoch 1630/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 6.2376 - val_loss: 0.5049\n",
      "Epoch 1631/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 2.9047 - val_loss: 0.5962\n",
      "Epoch 1632/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 3.4198 - val_loss: 0.8194\n",
      "Epoch 1633/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 2.2494 - val_loss: 0.9851\n",
      "Epoch 1634/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 6.0314 - val_loss: 0.8816\n",
      "Epoch 1635/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 7.9041 - val_loss: 0.7516\n",
      "Epoch 1636/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 3.0859 - val_loss: 0.7678\n",
      "Epoch 1637/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 3.0691 - val_loss: 0.7860\n",
      "Epoch 1638/2000\n",
      "81/81 [==============================] - 0s 240us/step - loss: 3.2668 - val_loss: 0.8241\n",
      "Epoch 1639/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 1.9511 - val_loss: 1.0282\n",
      "Epoch 1640/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.6906 - val_loss: 1.2437\n",
      "Epoch 1641/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 2.4811 - val_loss: 1.3581\n",
      "Epoch 1642/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 3.9855 - val_loss: 1.1656\n",
      "Epoch 1643/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 3.5559 - val_loss: 0.9021\n",
      "Epoch 1644/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 3.2587 - val_loss: 0.6740\n",
      "Epoch 1645/2000\n",
      "81/81 [==============================] - 0s 51us/step - loss: 1.2653 - val_loss: 0.4532\n",
      "Epoch 1646/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 2.0101 - val_loss: 0.3398\n",
      "Epoch 1647/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 6.6919 - val_loss: 0.3089\n",
      "Epoch 1648/2000\n",
      "81/81 [==============================] - 0s 147us/step - loss: 1.8283 - val_loss: 0.3406\n",
      "Epoch 1649/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 1.8763 - val_loss: 0.3740\n",
      "Epoch 1650/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 6.0781 - val_loss: 0.4531\n",
      "Epoch 1651/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 2.2301 - val_loss: 0.5346\n",
      "Epoch 1652/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 5.4632 - val_loss: 0.6965\n",
      "Epoch 1653/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 3.5849 - val_loss: 0.9757\n",
      "Epoch 1654/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 1.7105 - val_loss: 1.2310\n",
      "Epoch 1655/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 4.4841 - val_loss: 1.3917\n",
      "Epoch 1656/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 4.5079 - val_loss: 1.2680\n",
      "Epoch 1657/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 2.7836 - val_loss: 1.0074\n",
      "Epoch 1658/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 5.0386 - val_loss: 0.6277\n",
      "Epoch 1659/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 3.6142 - val_loss: 0.4144\n",
      "Epoch 1660/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 1.8126 - val_loss: 0.3171\n",
      "Epoch 1661/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 3.2603 - val_loss: 0.3359\n",
      "Epoch 1662/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 2.7780 - val_loss: 0.4676\n",
      "Epoch 1663/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 3.8639 - val_loss: 0.6320\n",
      "Epoch 1664/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 4.3611 - val_loss: 0.8666\n",
      "Epoch 1665/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 4.0257 - val_loss: 1.0668\n",
      "Epoch 1666/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 3.1486 - val_loss: 1.3033\n",
      "Epoch 1667/2000\n",
      "81/81 [==============================] - 0s 104us/step - loss: 1.9426 - val_loss: 1.3412\n",
      "Epoch 1668/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 2.7138 - val_loss: 1.2209\n",
      "Epoch 1669/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 3.1019 - val_loss: 0.9260\n",
      "Epoch 1670/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 2.2205 - val_loss: 0.6387\n",
      "Epoch 1671/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 2.8394 - val_loss: 0.5114\n",
      "Epoch 1672/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 1.6500 - val_loss: 0.4577\n",
      "Epoch 1673/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 2.6773 - val_loss: 0.4637\n",
      "Epoch 1674/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 1.7966 - val_loss: 0.5988\n",
      "Epoch 1675/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 2.5331 - val_loss: 0.7821\n",
      "Epoch 1676/2000\n",
      "81/81 [==============================] - 0s 128us/step - loss: 2.8008 - val_loss: 0.9641\n",
      "Epoch 1677/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 3.4236 - val_loss: 1.0757\n",
      "Epoch 1678/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 2.5728 - val_loss: 1.0661\n",
      "Epoch 1679/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 5.5083 - val_loss: 0.8603\n",
      "Epoch 1680/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 1.9044 - val_loss: 0.6378\n",
      "Epoch 1681/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 1.8113 - val_loss: 0.5232\n",
      "Epoch 1682/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 1.6280 - val_loss: 0.4219\n",
      "Epoch 1683/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 1.7139 - val_loss: 0.4175\n",
      "Epoch 1684/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 3.1883 - val_loss: 0.5217\n",
      "Epoch 1685/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 2.7277 - val_loss: 0.6753\n",
      "Epoch 1686/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 3.2679 - val_loss: 0.7438\n",
      "Epoch 1687/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.4426 - val_loss: 0.8181\n",
      "Epoch 1688/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.6310 - val_loss: 0.8830\n",
      "Epoch 1689/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 4.6656 - val_loss: 0.9816\n",
      "Epoch 1690/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 3.8425 - val_loss: 0.9407\n",
      "Epoch 1691/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 3.3920 - val_loss: 0.8027\n",
      "Epoch 1692/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 2.0452 - val_loss: 0.6719\n",
      "Epoch 1693/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 3.6639 - val_loss: 0.6559\n",
      "Epoch 1694/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 2.4806 - val_loss: 0.6153\n",
      "Epoch 1695/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 2.4981 - val_loss: 0.5884\n",
      "Epoch 1696/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 2.9214 - val_loss: 0.7005\n",
      "Epoch 1697/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 1.6912 - val_loss: 0.8083\n",
      "Epoch 1698/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 2.8353 - val_loss: 0.8818\n",
      "Epoch 1699/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 2.4051 - val_loss: 0.9287\n",
      "Epoch 1700/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 3.6155 - val_loss: 0.8477\n",
      "Epoch 1701/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 4.7385 - val_loss: 0.7407\n",
      "Epoch 1702/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 3.4094 - val_loss: 0.6057\n",
      "Epoch 1703/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 19.3135 - val_loss: 0.4873\n",
      "Epoch 1704/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 2.5397 - val_loss: 0.4852\n",
      "Epoch 1705/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 2.9064 - val_loss: 0.5246\n",
      "Epoch 1706/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 1.8432 - val_loss: 0.5361\n",
      "Epoch 1707/2000\n",
      "81/81 [==============================] - 0s 107us/step - loss: 1.6142 - val_loss: 0.5284\n",
      "Epoch 1708/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 1.7825 - val_loss: 0.5603\n",
      "Epoch 1709/2000\n",
      "81/81 [==============================] - 0s 97us/step - loss: 4.4460 - val_loss: 0.6081\n",
      "Epoch 1710/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 3.1863 - val_loss: 0.6008\n",
      "Epoch 1711/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 7.4257 - val_loss: 0.5372\n",
      "Epoch 1712/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 5.7233 - val_loss: 0.5164\n",
      "Epoch 1713/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 3.4757 - val_loss: 0.5531\n",
      "Epoch 1714/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 5.6101 - val_loss: 0.6529\n",
      "Epoch 1715/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 3.6904 - val_loss: 0.7523\n",
      "Epoch 1716/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 3.6175 - val_loss: 0.7782\n",
      "Epoch 1717/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.7709 - val_loss: 0.8523\n",
      "Epoch 1718/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 1.9613 - val_loss: 0.8114\n",
      "Epoch 1719/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 3.5817 - val_loss: 0.7405\n",
      "Epoch 1720/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 2.9256 - val_loss: 0.6752\n",
      "Epoch 1721/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 4.1308 - val_loss: 0.6537\n",
      "Epoch 1722/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 6.0412 - val_loss: 0.6269\n",
      "Epoch 1723/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 2.1112 - val_loss: 0.6680\n",
      "Epoch 1724/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 2.4534 - val_loss: 0.7066\n",
      "Epoch 1725/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 2.0696 - val_loss: 0.9102\n",
      "Epoch 1726/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 5.8614 - val_loss: 1.1782\n",
      "Epoch 1727/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 4.0932 - val_loss: 1.1837\n",
      "Epoch 1728/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 3.5868 - val_loss: 1.1337\n",
      "Epoch 1729/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.0743 - val_loss: 0.9058\n",
      "Epoch 1730/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 2.2717 - val_loss: 0.6091\n",
      "Epoch 1731/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 3.1924 - val_loss: 0.3607\n",
      "Epoch 1732/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 7.2226 - val_loss: 0.2443\n",
      "Epoch 1733/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 2.3707 - val_loss: 0.2703\n",
      "Epoch 1734/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 2.5668 - val_loss: 0.3750\n",
      "Epoch 1735/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 1.0822 - val_loss: 0.6119\n",
      "Epoch 1736/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 1.9958 - val_loss: 0.9313\n",
      "Epoch 1737/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 4.1159 - val_loss: 1.2958\n",
      "Epoch 1738/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.8419 - val_loss: 1.6107\n",
      "Epoch 1739/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 3.3809 - val_loss: 1.5893\n",
      "Epoch 1740/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 3.3946 - val_loss: 1.3478\n",
      "Epoch 1741/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 3.0875 - val_loss: 0.9443\n",
      "Epoch 1742/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 1.8122 - val_loss: 0.6239\n",
      "Epoch 1743/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 5.6010 - val_loss: 0.4107\n",
      "Epoch 1744/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 4.0433 - val_loss: 0.4216\n",
      "Epoch 1745/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 4.3763 - val_loss: 0.5541\n",
      "Epoch 1746/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 2.2349 - val_loss: 0.7683\n",
      "Epoch 1747/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 3.5118 - val_loss: 1.0284\n",
      "Epoch 1748/2000\n",
      "81/81 [==============================] - 0s 95us/step - loss: 2.5286 - val_loss: 1.1175\n",
      "Epoch 1749/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 2.6555 - val_loss: 1.2271\n",
      "Epoch 1750/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 1.7032 - val_loss: 1.1657\n",
      "Epoch 1751/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 4.4170 - val_loss: 0.9366\n",
      "Epoch 1752/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 4.1530 - val_loss: 0.7494\n",
      "Epoch 1753/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 2.1088 - val_loss: 0.5883\n",
      "Epoch 1754/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.8960 - val_loss: 0.4471\n",
      "Epoch 1755/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 2.5730 - val_loss: 0.4576\n",
      "Epoch 1756/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.5006 - val_loss: 0.5559\n",
      "Epoch 1757/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 1.7544 - val_loss: 0.6952\n",
      "Epoch 1758/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.1068 - val_loss: 0.8649\n",
      "Epoch 1759/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 3.4797 - val_loss: 1.0364\n",
      "Epoch 1760/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 2.7332 - val_loss: 1.1480\n",
      "Epoch 1761/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 4.3268 - val_loss: 1.2929\n",
      "Epoch 1762/2000\n",
      "81/81 [==============================] - 0s 122us/step - loss: 2.4631 - val_loss: 1.2366\n",
      "Epoch 1763/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 1.7757 - val_loss: 1.0505\n",
      "Epoch 1764/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 3.1738 - val_loss: 0.8471\n",
      "Epoch 1765/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 2.5707 - val_loss: 0.6942\n",
      "Epoch 1766/2000\n",
      "81/81 [==============================] - 0s 49us/step - loss: 1.6662 - val_loss: 0.6986\n",
      "Epoch 1767/2000\n",
      "81/81 [==============================] - 0s 91us/step - loss: 4.8899 - val_loss: 0.8674\n",
      "Epoch 1768/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 2.2391 - val_loss: 1.1234\n",
      "Epoch 1769/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.8967 - val_loss: 1.2567\n",
      "Epoch 1770/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 2.3582 - val_loss: 1.1680\n",
      "Epoch 1771/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 1.4833 - val_loss: 0.9603\n",
      "Epoch 1772/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 1.8319 - val_loss: 0.7494\n",
      "Epoch 1773/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 3.7426 - val_loss: 0.6080\n",
      "Epoch 1774/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 1.9876 - val_loss: 0.6013\n",
      "Epoch 1775/2000\n",
      "81/81 [==============================] - 0s 110us/step - loss: 4.5575 - val_loss: 0.5723\n",
      "Epoch 1776/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 3.7385 - val_loss: 0.6734\n",
      "Epoch 1777/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 3.2230 - val_loss: 0.7611\n",
      "Epoch 1778/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 1.5596 - val_loss: 0.9287\n",
      "Epoch 1779/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.1729 - val_loss: 1.1461\n",
      "Epoch 1780/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 4.3452 - val_loss: 1.2372\n",
      "Epoch 1781/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 2.5243 - val_loss: 1.2219\n",
      "Epoch 1782/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 2.2158 - val_loss: 1.0972\n",
      "Epoch 1783/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 2.4317 - val_loss: 0.8356\n",
      "Epoch 1784/2000\n",
      "81/81 [==============================] - 0s 108us/step - loss: 3.1175 - val_loss: 0.5642\n",
      "Epoch 1785/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 3.3397 - val_loss: 0.5286\n",
      "Epoch 1786/2000\n",
      "81/81 [==============================] - 0s 83us/step - loss: 2.6784 - val_loss: 0.6205\n",
      "Epoch 1787/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 1.4953 - val_loss: 0.7349\n",
      "Epoch 1788/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 5.1329 - val_loss: 0.8555\n",
      "Epoch 1789/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 2.1276 - val_loss: 0.9613\n",
      "Epoch 1790/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 3.5935 - val_loss: 0.9535\n",
      "Epoch 1791/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 1.7338 - val_loss: 0.9203\n",
      "Epoch 1792/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 3.0708 - val_loss: 0.8452\n",
      "Epoch 1793/2000\n",
      "81/81 [==============================] - 0s 107us/step - loss: 2.7953 - val_loss: 0.7934\n",
      "Epoch 1794/2000\n",
      "81/81 [==============================] - 0s 120us/step - loss: 1.8357 - val_loss: 0.7549\n",
      "Epoch 1795/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 3.9078 - val_loss: 0.7711\n",
      "Epoch 1796/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 2.9200 - val_loss: 0.7364\n",
      "Epoch 1797/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 3.4865 - val_loss: 0.7389\n",
      "Epoch 1798/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 2.6915 - val_loss: 0.7190\n",
      "Epoch 1799/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.6073 - val_loss: 0.7385\n",
      "Epoch 1800/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 2.1046 - val_loss: 0.7498\n",
      "Epoch 1801/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 3.9950 - val_loss: 0.7454\n",
      "Epoch 1802/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 4.5687 - val_loss: 0.6787\n",
      "Epoch 1803/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 3.2440 - val_loss: 0.5914\n",
      "Epoch 1804/2000\n",
      "81/81 [==============================] - 0s 106us/step - loss: 1.7164 - val_loss: 0.6089\n",
      "Epoch 1805/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 1.4117 - val_loss: 0.7271\n",
      "Epoch 1806/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 1.7672 - val_loss: 0.8429\n",
      "Epoch 1807/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 1.9354 - val_loss: 0.9011\n",
      "Epoch 1808/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 1.3058 - val_loss: 0.8027\n",
      "Epoch 1809/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 2.8793 - val_loss: 0.6412\n",
      "Epoch 1810/2000\n",
      "81/81 [==============================] - 0s 87us/step - loss: 2.6774 - val_loss: 0.5418\n",
      "Epoch 1811/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 1.8862 - val_loss: 0.5952\n",
      "Epoch 1812/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.2388 - val_loss: 0.7248\n",
      "Epoch 1813/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 4.2706 - val_loss: 0.9894\n",
      "Epoch 1814/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.0146 - val_loss: 1.1020\n",
      "Epoch 1815/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.9617 - val_loss: 1.0900\n",
      "Epoch 1816/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 2.0666 - val_loss: 0.8839\n",
      "Epoch 1817/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 1.7802 - val_loss: 0.6125\n",
      "Epoch 1818/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 2.3415 - val_loss: 0.3835\n",
      "Epoch 1819/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 3.4545 - val_loss: 0.3168\n",
      "Epoch 1820/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 4.4253 - val_loss: 0.3626\n",
      "Epoch 1821/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 1.6509 - val_loss: 0.5384\n",
      "Epoch 1822/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 1.1180 - val_loss: 0.7714\n",
      "Epoch 1823/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 4.1262 - val_loss: 0.8613\n",
      "Epoch 1824/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 4.4139 - val_loss: 0.8978\n",
      "Epoch 1825/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 2.3782 - val_loss: 0.9224\n",
      "Epoch 1826/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 4.0507 - val_loss: 0.7968\n",
      "Epoch 1827/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 1.7810 - val_loss: 0.6815\n",
      "Epoch 1828/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 1.4528 - val_loss: 0.6702\n",
      "Epoch 1829/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.3634 - val_loss: 0.7565\n",
      "Epoch 1830/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 3.5505 - val_loss: 0.7931\n",
      "Epoch 1831/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 2.5355 - val_loss: 0.8087\n",
      "Epoch 1832/2000\n",
      "81/81 [==============================] - 0s 105us/step - loss: 1.4220 - val_loss: 0.7608\n",
      "Epoch 1833/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 2.3415 - val_loss: 0.8256\n",
      "Epoch 1834/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 3.2558 - val_loss: 0.7425\n",
      "Epoch 1835/2000\n",
      "81/81 [==============================] - 0s 81us/step - loss: 1.3725 - val_loss: 0.7082\n",
      "Epoch 1836/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 2.3470 - val_loss: 0.7171\n",
      "Epoch 1837/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 1.8385 - val_loss: 0.6846\n",
      "Epoch 1838/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 2.9665 - val_loss: 0.7079\n",
      "Epoch 1839/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 3.2376 - val_loss: 0.6745\n",
      "Epoch 1840/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 2.7416 - val_loss: 0.7080\n",
      "Epoch 1841/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 1.9760 - val_loss: 0.6790\n",
      "Epoch 1842/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 7.6630 - val_loss: 0.5743\n",
      "Epoch 1843/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.6689 - val_loss: 0.5597\n",
      "Epoch 1844/2000\n",
      "81/81 [==============================] - 0s 75us/step - loss: 4.6300 - val_loss: 0.5212\n",
      "Epoch 1845/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 1.2269 - val_loss: 0.5299\n",
      "Epoch 1846/2000\n",
      "81/81 [==============================] - 0s 146us/step - loss: 2.4435 - val_loss: 0.6662\n",
      "Epoch 1847/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 1.9164 - val_loss: 0.7692\n",
      "Epoch 1848/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 2.6134 - val_loss: 0.9409\n",
      "Epoch 1849/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 1.9601 - val_loss: 1.0591\n",
      "Epoch 1850/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.0685 - val_loss: 0.8627\n",
      "Epoch 1851/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 2.2596 - val_loss: 0.7615\n",
      "Epoch 1852/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 6.6586 - val_loss: 0.7577\n",
      "Epoch 1853/2000\n",
      "81/81 [==============================] - 0s 96us/step - loss: 1.5338 - val_loss: 0.8147\n",
      "Epoch 1854/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 1.3492 - val_loss: 0.8560\n",
      "Epoch 1855/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 2.5493 - val_loss: 0.8218\n",
      "Epoch 1856/2000\n",
      "81/81 [==============================] - 0s 68us/step - loss: 1.1745 - val_loss: 0.7856\n",
      "Epoch 1857/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 6.1717 - val_loss: 0.6585\n",
      "Epoch 1858/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 1.7151 - val_loss: 0.6116\n",
      "Epoch 1859/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 1.3677 - val_loss: 0.5984\n",
      "Epoch 1860/2000\n",
      "81/81 [==============================] - 0s 101us/step - loss: 3.2001 - val_loss: 0.7882\n",
      "Epoch 1861/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 1.4062 - val_loss: 1.0363\n",
      "Epoch 1862/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.2745 - val_loss: 1.1891\n",
      "Epoch 1863/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 2.4498 - val_loss: 1.1139\n",
      "Epoch 1864/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 2.9746 - val_loss: 0.9716\n",
      "Epoch 1865/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 2.5913 - val_loss: 0.7561\n",
      "Epoch 1866/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 1.1904 - val_loss: 0.5631\n",
      "Epoch 1867/2000\n",
      "81/81 [==============================] - 0s 79us/step - loss: 3.6137 - val_loss: 0.4722\n",
      "Epoch 1868/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 7.1259 - val_loss: 0.5528\n",
      "Epoch 1869/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 1.5395 - val_loss: 0.6419\n",
      "Epoch 1870/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 3.7603 - val_loss: 0.8903\n",
      "Epoch 1871/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 2.7442 - val_loss: 1.0823\n",
      "Epoch 1872/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 2.4988 - val_loss: 1.1641\n",
      "Epoch 1873/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 2.1514 - val_loss: 1.1633\n",
      "Epoch 1874/2000\n",
      "81/81 [==============================] - 0s 115us/step - loss: 3.0614 - val_loss: 0.9701\n",
      "Epoch 1875/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 2.0019 - val_loss: 0.6716\n",
      "Epoch 1876/2000\n",
      "81/81 [==============================] - 0s 82us/step - loss: 2.6194 - val_loss: 0.4860\n",
      "Epoch 1877/2000\n",
      "81/81 [==============================] - 0s 119us/step - loss: 3.4804 - val_loss: 0.4298\n",
      "Epoch 1878/2000\n",
      "81/81 [==============================] - 0s 88us/step - loss: 2.8853 - val_loss: 0.4417\n",
      "Epoch 1879/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 1.5853 - val_loss: 0.5141\n",
      "Epoch 1880/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 1.8426 - val_loss: 0.6538\n",
      "Epoch 1881/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 2.3423 - val_loss: 0.8929\n",
      "Epoch 1882/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 2.4917 - val_loss: 1.1855\n",
      "Epoch 1883/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 5.9518 - val_loss: 1.4680\n",
      "Epoch 1884/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 1.6587 - val_loss: 1.4290\n",
      "Epoch 1885/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 6.5197 - val_loss: 1.0975\n",
      "Epoch 1886/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 1.5899 - val_loss: 0.7701\n",
      "Epoch 1887/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 4.4298 - val_loss: 0.5340\n",
      "Epoch 1888/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 2.1780 - val_loss: 0.4275\n",
      "Epoch 1889/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 3.0652 - val_loss: 0.4608\n",
      "Epoch 1890/2000\n",
      "81/81 [==============================] - 0s 102us/step - loss: 1.4221 - val_loss: 0.5408\n",
      "Epoch 1891/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 2.6435 - val_loss: 0.6457\n",
      "Epoch 1892/2000\n",
      "81/81 [==============================] - 0s 125us/step - loss: 2.0262 - val_loss: 0.9061\n",
      "Epoch 1893/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 2.7152 - val_loss: 1.2403\n",
      "Epoch 1894/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 1.3346 - val_loss: 1.3697\n",
      "Epoch 1895/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 2.2738 - val_loss: 1.2322\n",
      "Epoch 1896/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 1.6455 - val_loss: 0.9378\n",
      "Epoch 1897/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 2.1934 - val_loss: 0.6489\n",
      "Epoch 1898/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 1.6359 - val_loss: 0.5008\n",
      "Epoch 1899/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 2.9814 - val_loss: 0.5449\n",
      "Epoch 1900/2000\n",
      "81/81 [==============================] - 0s 54us/step - loss: 1.9773 - val_loss: 0.6453\n",
      "Epoch 1901/2000\n",
      "81/81 [==============================] - 0s 69us/step - loss: 1.3683 - val_loss: 0.7746\n",
      "Epoch 1902/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 1.5896 - val_loss: 0.9137\n",
      "Epoch 1903/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 4.3802 - val_loss: 1.0395\n",
      "Epoch 1904/2000\n",
      "81/81 [==============================] - 0s 102us/step - loss: 2.2834 - val_loss: 0.9324\n",
      "Epoch 1905/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.1245 - val_loss: 0.7962\n",
      "Epoch 1906/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 2.5357 - val_loss: 0.8116\n",
      "Epoch 1907/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 3.5354 - val_loss: 0.7718\n",
      "Epoch 1908/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 4.4195 - val_loss: 0.7495\n",
      "Epoch 1909/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 5.2822 - val_loss: 0.5825\n",
      "Epoch 1910/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 1.2899 - val_loss: 0.5011\n",
      "Epoch 1911/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 3.5121 - val_loss: 0.5319\n",
      "Epoch 1912/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.5250 - val_loss: 0.7025\n",
      "Epoch 1913/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 2.2353 - val_loss: 0.8282\n",
      "Epoch 1914/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 2.3643 - val_loss: 0.8582\n",
      "Epoch 1915/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 2.4308 - val_loss: 0.8968\n",
      "Epoch 1916/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 2.3160 - val_loss: 0.8932\n",
      "Epoch 1917/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 1.8379 - val_loss: 0.8957\n",
      "Epoch 1918/2000\n",
      "81/81 [==============================] - 0s 129us/step - loss: 1.5413 - val_loss: 0.7911\n",
      "Epoch 1919/2000\n",
      "81/81 [==============================] - 0s 142us/step - loss: 2.3297 - val_loss: 0.7128\n",
      "Epoch 1920/2000\n",
      "81/81 [==============================] - 0s 109us/step - loss: 3.9478 - val_loss: 0.6626\n",
      "Epoch 1921/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 2.3838 - val_loss: 0.6684\n",
      "Epoch 1922/2000\n",
      "81/81 [==============================] - 0s 84us/step - loss: 3.6672 - val_loss: 0.6861\n",
      "Epoch 1923/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 2.2402 - val_loss: 0.7675\n",
      "Epoch 1924/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 2.0483 - val_loss: 0.7487\n",
      "Epoch 1925/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 1.3186 - val_loss: 0.7703\n",
      "Epoch 1926/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 4.1558 - val_loss: 0.9190\n",
      "Epoch 1927/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 2.8193 - val_loss: 1.0603\n",
      "Epoch 1928/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 2.3736 - val_loss: 1.0299\n",
      "Epoch 1929/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 1.9511 - val_loss: 0.8473\n",
      "Epoch 1930/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 1.7379 - val_loss: 0.6850\n",
      "Epoch 1931/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 2.6939 - val_loss: 0.6041\n",
      "Epoch 1932/2000\n",
      "81/81 [==============================] - 0s 119us/step - loss: 2.2633 - val_loss: 0.5517\n",
      "Epoch 1933/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 3.0987 - val_loss: 0.5470\n",
      "Epoch 1934/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 1.2284 - val_loss: 0.6632\n",
      "Epoch 1935/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.1210 - val_loss: 0.8271\n",
      "Epoch 1936/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 3.2547 - val_loss: 0.9587\n",
      "Epoch 1937/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 1.2534 - val_loss: 0.9747\n",
      "Epoch 1938/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 1.5575 - val_loss: 0.8896\n",
      "Epoch 1939/2000\n",
      "81/81 [==============================] - 0s 147us/step - loss: 1.4599 - val_loss: 0.7924\n",
      "Epoch 1940/2000\n",
      "81/81 [==============================] - 0s 92us/step - loss: 1.7964 - val_loss: 0.7634\n",
      "Epoch 1941/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 1.0353 - val_loss: 0.7498\n",
      "Epoch 1942/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 1.2611 - val_loss: 0.6562\n",
      "Epoch 1943/2000\n",
      "81/81 [==============================] - 0s 100us/step - loss: 4.6492 - val_loss: 0.6163\n",
      "Epoch 1944/2000\n",
      "81/81 [==============================] - 0s 90us/step - loss: 3.3927 - val_loss: 0.6251\n",
      "Epoch 1945/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 1.7179 - val_loss: 0.6649\n",
      "Epoch 1946/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 1.2212 - val_loss: 0.6673\n",
      "Epoch 1947/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 1.8905 - val_loss: 0.7387\n",
      "Epoch 1948/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 2.0751 - val_loss: 0.7887\n",
      "Epoch 1949/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 1.4340 - val_loss: 0.8337\n",
      "Epoch 1950/2000\n",
      "81/81 [==============================] - 0s 61us/step - loss: 1.8042 - val_loss: 0.6719\n",
      "Epoch 1951/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 2.2439 - val_loss: 0.6684\n",
      "Epoch 1952/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 4.4773 - val_loss: 0.8232\n",
      "Epoch 1953/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 1.8182 - val_loss: 0.9687\n",
      "Epoch 1954/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 3.7640 - val_loss: 1.1252\n",
      "Epoch 1955/2000\n",
      "81/81 [==============================] - 0s 77us/step - loss: 2.9806 - val_loss: 1.1132\n",
      "Epoch 1956/2000\n",
      "81/81 [==============================] - 0s 66us/step - loss: 3.7561 - val_loss: 1.0034\n",
      "Epoch 1957/2000\n",
      "81/81 [==============================] - 0s 62us/step - loss: 1.2725 - val_loss: 0.7803\n",
      "Epoch 1958/2000\n",
      "81/81 [==============================] - 0s 59us/step - loss: 1.7605 - val_loss: 0.5490\n",
      "Epoch 1959/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 2.4773 - val_loss: 0.4447\n",
      "Epoch 1960/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 1.9836 - val_loss: 0.4564\n",
      "Epoch 1961/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 4.8604 - val_loss: 0.5310\n",
      "Epoch 1962/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 1.2760 - val_loss: 0.6270\n",
      "Epoch 1963/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 1.6893 - val_loss: 0.8720\n",
      "Epoch 1964/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 1.3700 - val_loss: 1.1190\n",
      "Epoch 1965/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 2.1146 - val_loss: 1.1084\n",
      "Epoch 1966/2000\n",
      "81/81 [==============================] - 0s 65us/step - loss: 4.8127 - val_loss: 0.9073\n",
      "Epoch 1967/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 3.3718 - val_loss: 0.7054\n",
      "Epoch 1968/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 1.9031 - val_loss: 0.5830\n",
      "Epoch 1969/2000\n",
      "81/81 [==============================] - 0s 115us/step - loss: 1.5634 - val_loss: 0.5372\n",
      "Epoch 1970/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 1.9047 - val_loss: 0.4706\n",
      "Epoch 1971/2000\n",
      "81/81 [==============================] - 0s 89us/step - loss: 1.6280 - val_loss: 0.5140\n",
      "Epoch 1972/2000\n",
      "81/81 [==============================] - 0s 85us/step - loss: 3.4905 - val_loss: 0.6304\n",
      "Epoch 1973/2000\n",
      "81/81 [==============================] - 0s 55us/step - loss: 2.3923 - val_loss: 0.6847\n",
      "Epoch 1974/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 3.0578 - val_loss: 0.7969\n",
      "Epoch 1975/2000\n",
      "81/81 [==============================] - 0s 80us/step - loss: 8.1685 - val_loss: 0.8815\n",
      "Epoch 1976/2000\n",
      "81/81 [==============================] - 0s 52us/step - loss: 2.3428 - val_loss: 1.0055\n",
      "Epoch 1977/2000\n",
      "81/81 [==============================] - 0s 73us/step - loss: 2.0844 - val_loss: 1.0333\n",
      "Epoch 1978/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 2.3657 - val_loss: 0.9944\n",
      "Epoch 1979/2000\n",
      "81/81 [==============================] - 0s 56us/step - loss: 1.5051 - val_loss: 0.8777\n",
      "Epoch 1980/2000\n",
      "81/81 [==============================] - 0s 70us/step - loss: 0.9602 - val_loss: 0.6787\n",
      "Epoch 1981/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 1.3409 - val_loss: 0.5548\n",
      "Epoch 1982/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 1.9273 - val_loss: 0.5156\n",
      "Epoch 1983/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 1.1843 - val_loss: 0.5488\n",
      "Epoch 1984/2000\n",
      "81/81 [==============================] - 0s 53us/step - loss: 3.2191 - val_loss: 0.7586\n",
      "Epoch 1985/2000\n",
      "81/81 [==============================] - 0s 78us/step - loss: 0.9734 - val_loss: 0.9492\n",
      "Epoch 1986/2000\n",
      "81/81 [==============================] - 0s 72us/step - loss: 2.1607 - val_loss: 0.9845\n",
      "Epoch 1987/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 1.4543 - val_loss: 0.9257\n",
      "Epoch 1988/2000\n",
      "81/81 [==============================] - 0s 64us/step - loss: 1.6955 - val_loss: 0.8654\n",
      "Epoch 1989/2000\n",
      "81/81 [==============================] - 0s 58us/step - loss: 2.8680 - val_loss: 0.7132\n",
      "Epoch 1990/2000\n",
      "81/81 [==============================] - 0s 67us/step - loss: 4.5130 - val_loss: 0.5951\n",
      "Epoch 1991/2000\n",
      "81/81 [==============================] - 0s 63us/step - loss: 2.7574 - val_loss: 0.6367\n",
      "Epoch 1992/2000\n",
      "81/81 [==============================] - 0s 76us/step - loss: 1.6223 - val_loss: 0.7553\n",
      "Epoch 1993/2000\n",
      "81/81 [==============================] - 0s 94us/step - loss: 1.9936 - val_loss: 0.8803\n",
      "Epoch 1994/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 1.1544 - val_loss: 1.0089\n",
      "Epoch 1995/2000\n",
      "81/81 [==============================] - 0s 93us/step - loss: 2.4927 - val_loss: 1.0753\n",
      "Epoch 1996/2000\n",
      "81/81 [==============================] - 0s 86us/step - loss: 1.9019 - val_loss: 1.0248\n",
      "Epoch 1997/2000\n",
      "81/81 [==============================] - 0s 74us/step - loss: 2.1055 - val_loss: 0.8276\n",
      "Epoch 1998/2000\n",
      "81/81 [==============================] - 0s 71us/step - loss: 2.9108 - val_loss: 0.6253\n",
      "Epoch 1999/2000\n",
      "81/81 [==============================] - 0s 60us/step - loss: 2.8841 - val_loss: 0.5170\n",
      "Epoch 2000/2000\n",
      "81/81 [==============================] - 0s 57us/step - loss: 2.0738 - val_loss: 0.5164\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(1, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"linear\"))\n",
    "model.summary()\n",
    "# compiling the model\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"mean_squared_error\"\n",
    ")\n",
    "\n",
    "results = model.fit(\n",
    " train_x, train_y,\n",
    " epochs= 2000,\n",
    " batch_size = 200,\n",
    " validation_data = (test_x, test_y)\n",
    ")\n",
    "a=model._make_predict_function()\n",
    "#print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.63979  ]\n",
      " [2.299243 ]\n",
      " [2.332882 ]\n",
      " [2.7554088]\n",
      " [2.4253745]\n",
      " [2.8521085]\n",
      " [2.9172692]]\n"
     ]
    }
   ],
   "source": [
    "Xnew = [[1232,1070,1086,1287,1130,1333,1364]]\n",
    "ynew = model.predict(Xnew)\n",
    "print(ynew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('carnegie_mellon.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa=data.iloc[:,0]\n",
    "sat=data.iloc[:,1]\n",
    "results=data.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(len(gpa)):\n",
    "    if results[i]==0:\n",
    "        plt.scatter(sat[i],gpa[i],c='r')\n",
    "    else:\n",
    "        plt.scatter(sat[i],gpa[i],c='g')\n",
    "ax.set_xlabel(\"GPA\")\n",
    "ax.set_ylabel(\"SAT\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=2,init='k-means++',max_iter=500, tol=0.0001).fit(df)\n",
    "classier=kmeans.labels_\n",
    "classier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3.86625   , 1928.4375    ],\n",
       "       [   3.98342857, 2230.71428571]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "red=[]\n",
    "green=[]\n",
    "redy=[]\n",
    "greeny=[]\n",
    "redcount=0\n",
    "greencount=0\n",
    "for i in range(len(gpa)):\n",
    "    if classier[i]==0:\n",
    "        plt.scatter(sat[i],gpa[i],color='red')\n",
    "        red.append(sat[i])\n",
    "        redy.append(gpa[i])\n",
    "        redcount+=1\n",
    "    else:\n",
    "        plt.scatter(sat[i],gpa[i],color='green') \n",
    "        green.append(sat[i])\n",
    "        greeny.append(gpa[i])\n",
    "        greencount+=1\n",
    "plt.xlabel('SAT')\n",
    "plt.ylabel('University GPA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat1=np.arange(1034,1137.3,0.02)\n",
    "sat2=np.arange(1089.95,1232,0.02)\n",
    "sat3=np.arange(1242,1373.85,0.02)\n",
    "sat4=np.arange(1329.9,1450,0.02)\n",
    "gpa=np.arange(2,4,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
